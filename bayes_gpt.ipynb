{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "EagsX3TPhIqb"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "import inspect\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import pandas as pd\n",
        "import requests\n",
        "import tiktoken\n",
        "import time\n",
        "import torch\n",
        "import torch.distributed as dist\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torchbnn as bnn\n",
        "\n",
        "from dataclasses import dataclass\n",
        "from torch.distributed import init_process_group, destroy_process_group\n",
        "from torch.nn.parallel import DistributedDataParallel as DDP\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from tqdm import tqdm\n",
        "from transformers import GPT2LMHeadModel"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fdbYB7nX1RyC"
      },
      "source": [
        "## Model Architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "M3QmzgFZnDH6"
      },
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class GPT2Config:\n",
        "    block_size: int = 1024\n",
        "    vocab_size: int = 50257\n",
        "    n_layer: int = 12\n",
        "    n_head: int = 12\n",
        "    n_embd: int = 768"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "4nX7N53ahQNg"
      },
      "outputs": [],
      "source": [
        "class BayesMultiHeadAttention(nn.Module):\n",
        "    def __init__(self, config: GPT2Config):\n",
        "        super(BayesMultiHeadAttention, self).__init__()\n",
        "        assert config.n_embd % config.n_head == 0\n",
        "        # self.c_attn = bnn.BayesLinear(prior_mu=0, prior_sigma=0.005, in_features=config.n_embd, out_features=3*config.n_embd)\n",
        "        self.c_attn = nn.Linear(config.n_embd, 3*config.n_embd)\n",
        "        # self.c_proj = bnn.BayesLinear(prior_mu=0, prior_sigma=0.005, in_features=config.n_embd, out_features=config.n_embd)\n",
        "        self.c_proj = nn.Linear(config.n_embd, config.n_embd)\n",
        "        self.n_head = config.n_head\n",
        "        self.n_embd = config.n_embd\n",
        "        self.flash = hasattr(torch.nn.functional, 'scaled_dot_product_attention')\n",
        "        if not self.flash:\n",
        "            self.register_buffer('mask', torch.tril(torch.ones(config.block_size, config.block_size)\n",
        "                                .view(1, 1, config.block_size, config.block_size)))\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        B, T, C = x.size()\n",
        "        q, k, v = self.c_attn(x).split(self.n_embd, dim=2)\n",
        "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
        "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
        "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
        "        \n",
        "        if self.flash:\n",
        "            y = torch.nn.functional.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=0.0, is_causal=True)\n",
        "        else:\n",
        "            att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
        "            att = att.masked_fill(self.mask[:, :, :T, :T] == 0, float('-inf'))\n",
        "            att = F.softmax(att, dim=-1)\n",
        "            y = att @ v\n",
        "        \n",
        "        y = y.transpose(1, 2).contiguous().view(B, T, C)\n",
        "        y = self.c_proj(y)\n",
        "        return y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "HK4nIVwAlNXu"
      },
      "outputs": [],
      "source": [
        "class BayesMLP(nn.Module):\n",
        "    def __init__(self, config: GPT2Config):\n",
        "        super(BayesMLP, self).__init__()\n",
        "        self.c_fc = bnn.BayesLinear(prior_mu=0, prior_sigma=0.005, in_features=config.n_embd, out_features=4*config.n_embd)\n",
        "        self.gelu = nn.GELU(approximate='tanh')\n",
        "        self.c_proj = bnn.BayesLinear(prior_mu=0, prior_sigma=0.005, in_features=4*config.n_embd, out_features=config.n_embd)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        x = self.c_fc(x)\n",
        "        x = self.gelu(x)\n",
        "        x = self.c_proj(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "foi0xI9QkBAI"
      },
      "outputs": [],
      "source": [
        "class BayesBlock(nn.Module):\n",
        "    def __init__(self, config: GPT2Config):\n",
        "        super(BayesBlock, self).__init__()\n",
        "\n",
        "        self.ln_1 = nn.LayerNorm(config.n_embd)\n",
        "        self.attn = BayesMultiHeadAttention(config)\n",
        "        self.ln_2 = nn.LayerNorm(config.n_embd)\n",
        "        self.mlp = BayesMLP(config)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        x = x + self.attn(self.ln_1(x))\n",
        "        x = x + self.mlp(self.ln_2(x))\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "32U-xrNMm9v_"
      },
      "outputs": [],
      "source": [
        "class BayesGPT(nn.Module):\n",
        "    def __init__(self, config: GPT2Config):\n",
        "        super(BayesGPT, self).__init__()\n",
        "        self.config = config\n",
        "\n",
        "        self.transformer = nn.ModuleDict(dict(\n",
        "            wte = nn.Embedding(config.vocab_size, config.n_embd),\n",
        "            wpe = nn.Embedding(config.block_size, config.n_embd),\n",
        "            h = nn.ModuleList([BayesBlock(config) for _ in range(config.n_layer)]),\n",
        "            ln_f = nn.LayerNorm(config.n_embd),\n",
        "        ))\n",
        "        self.lm_head = bnn.BayesLinear(prior_mu=0, prior_sigma=0.005, in_features=config.n_embd, out_features=config.vocab_size, bias=False)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        _, T = x.size()\n",
        "        assert T <= self.config.block_size, f'Cannot forward sequence of length {T}, block size is only {self.config.block_size}'\n",
        "        pos = torch.arange(0, T, dtype=torch.long, device=x.device)\n",
        "        pos_emb = self.transformer.wpe(pos)\n",
        "        tok_emb = self.transformer.wte(x)\n",
        "        x = tok_emb + pos_emb\n",
        "        for block in self.transformer.h:\n",
        "            x = block(x)\n",
        "        x = self.transformer.ln_f(x)\n",
        "        logits = self.lm_head(x)\n",
        "        return logits\n",
        "\n",
        "    def configure_optimizers(self, weight_decay: float, learning_rate: float, device_type: str) -> optim.AdamW:\n",
        "        param_dict = {pn: p for pn, p in self.named_parameters()}\n",
        "        param_dict = {pn: p for pn, p in param_dict.items() if p.requires_grad}\n",
        "        decay_params = [p for _, p in param_dict.items() if p.dim() >= 2]\n",
        "        nodecay_params = [p for _, p in param_dict.items() if p.dim() < 2]\n",
        "        optim_groups = [\n",
        "            {'params': decay_params, 'weight_decay': weight_decay},\n",
        "            {'params': nodecay_params, 'weight_decay': 0.0}\n",
        "        ]\n",
        "        fused_available = 'fused' in inspect.signature(optim.AdamW).parameters\n",
        "        use_fused = fused_available and device_type == 'cuda'\n",
        "        optimizer = optim.AdamW(optim_groups, lr=learning_rate, betas=(0.9, 0.95), eps=1e-8, fused=use_fused)\n",
        "        return optimizer\n",
        "\n",
        "    def load_pretrained_model(self, pretrained_model: nn.Module) -> None:\n",
        "        with torch.no_grad():\n",
        "            for name, param in pretrained_model.state_dict().items():\n",
        "                if 'logvar' in name or 'mask' in name:\n",
        "                    continue\n",
        "                if name == 'lm_head.weight':\n",
        "                    self.state_dict()['lm_head.weight_mu'].copy_(param)\n",
        "                elif 'attn.c_attn.bias' in name:\n",
        "                    # self.state_dict()[name.replace('c_attn.bias', 'c_attn.bias_mu')].copy_(param)\n",
        "                    self.state_dict()[name].copy_(param)\n",
        "                elif 'attn.c_attn.weight' in name:\n",
        "                    # self.state_dict()[name.replace('c_attn.weight', 'c_attn.weight_mu')].copy_(param.T)\n",
        "                    self.state_dict()[name].copy_(param.T)\n",
        "                elif 'attn.c_proj.bias' in name:\n",
        "                    # self.state_dict()[name.replace('c_proj.bias', 'c_proj.bias_mu')].copy_(param)\n",
        "                    self.state_dict()[name].copy_(param)\n",
        "                elif 'attn.c_proj.weight' in name:\n",
        "                    # self.state_dict()[name.replace('c_proj.weight', 'c_proj.weight_mu')].copy_(param.T)\n",
        "                    self.state_dict()[name].copy_(param.T)\n",
        "                elif 'mlp.c_proj.bias' in name:\n",
        "                    self.state_dict()[name.replace('c_proj.bias', 'c_proj.bias_mu')].copy_(param)\n",
        "                elif 'mlp.c_proj.weight' in name:\n",
        "                    self.state_dict()[name.replace('c_proj.weight', 'c_proj.weight_mu')].copy_(param.T)\n",
        "                elif 'mlp.c_fc.bias' in name:\n",
        "                    self.state_dict()[name.replace('c_fc.bias', 'c_fc.bias_mu')].copy_(param)\n",
        "                elif 'mlp.c_fc.weight' in name:\n",
        "                    self.state_dict()[name.replace('c_fc.weight', 'c_fc.weight_mu')].copy_(param.T)\n",
        "                else:\n",
        "                    self.state_dict()[name].copy_(param)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NIoTo-7i5Dsu"
      },
      "source": [
        "## Data Processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FQg96__ouMVN",
        "outputId": "7600f5fc-80b1-47dc-e556-95c17b97fa15"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cpu\n"
          ]
        }
      ],
      "source": [
        "tokenizer = tiktoken.get_encoding('gpt2')\n",
        "ddp = int(os.environ.get('RANK', -1)) != -1\n",
        "if ddp:\n",
        "    assert torch.cuda.is_available()\n",
        "    init_process_group(backend='nccl')\n",
        "    ddp_rank = int(os.environ['RANK'])\n",
        "    ddp_local_rank = int(os.environ['LOCAL_RANK'])\n",
        "    ddp_world_size = int(os.environ['WORLD_SIZE'])\n",
        "    device = f'cuda:{ddp_local_rank}'\n",
        "    torch.cuda.set_device(device)\n",
        "    master_process = ddp_rank == 0\n",
        "else:\n",
        "    ddp_rank = 0\n",
        "    ddp_local_rank = 0\n",
        "    ddp_world_size = 1\n",
        "    master_process = True\n",
        "    device = 'cpu'\n",
        "    if torch.cuda.is_available():\n",
        "        device = 'cuda'\n",
        "    print(f'Using device: {device}')\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    device_type = 'cuda'\n",
        "else:\n",
        "    device_type = 'cpu'\n",
        "\n",
        "device = torch.device(device_type)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Wn2iynQ7FWnV"
      },
      "outputs": [],
      "source": [
        "def filter_short_texts(df: pd.DataFrame, tokenizer: tiktoken.Encoding, min_length: int) -> pd.DataFrame:\n",
        "    df['encoded_length'] = df['text'].apply(lambda x: len(tokenizer.encode(x)))\n",
        "    mask = df['encoded_length'] >= min_length\n",
        "    filtered_df = df[mask].drop(columns=['encoded_length'])\n",
        "    return filtered_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "FKrfa23qKOYg"
      },
      "outputs": [],
      "source": [
        "subdir = 'data'\n",
        "if not os.path.exists(subdir):\n",
        "    os.makedirs(subdir)\n",
        "    \n",
        "file_options = [\n",
        "    'webtext',\n",
        "    'small-117M',  'small-117M-k40',\n",
        "    'medium-345M', 'medium-345M-k40',\n",
        "    'large-762M',  'large-762M-k40',\n",
        "    'xl-1542M',    'xl-1542M-k40',\n",
        "]\n",
        "\n",
        "file_option = file_options[0]\n",
        "\n",
        "for split in ['train', 'valid', 'test']:\n",
        "    filename = file_option + '.' + split + '.jsonl'\n",
        "    if not os.path.exists(os.path.join(subdir, filename)):\n",
        "      r = requests.get('https://openaipublic.azureedge.net/gpt-2/output-dataset/v1/' + filename, stream=True)\n",
        "\n",
        "      with open(os.path.join(subdir, filename), 'wb') as f:\n",
        "          file_size = int(r.headers['content-length'])\n",
        "          chunk_size = 1000\n",
        "          with tqdm(ncols=100, desc='Fetching ' + filename, total=file_size, unit_scale=True) as pbar:\n",
        "              for chunk in r.iter_content(chunk_size=chunk_size):\n",
        "                  f.write(chunk)\n",
        "                  pbar.update(chunk_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "5PLiFq1jyywA"
      },
      "outputs": [],
      "source": [
        "class TextDataset(Dataset):\n",
        "    def __init__(self, texts: pd.DataFrame, tokenizer: tiktoken.Encoding, block_size: int = 128):\n",
        "        self.texts = texts\n",
        "        self.tokenizer = tokenizer\n",
        "        self.block_size = block_size\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx: int) -> tuple[torch.Tensor, torch.Tensor]:\n",
        "        text = self.texts.iloc[idx]\n",
        "        tokens = self.tokenizer.encode(text)\n",
        "        chunk = tokens[:self.block_size]\n",
        "        input_ids = chunk[:-1]\n",
        "        target_ids = chunk[1:]\n",
        "        return torch.tensor(input_ids, dtype=torch.long), torch.tensor(target_ids, dtype=torch.long)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "block_size = 128\n",
        "\n",
        "try:\n",
        "    train_df = pd.read_json(f'{os.getcwd()}/{subdir}/{file_option}.train.jsonl', lines=True)\n",
        "    # this is for simplicity so each row of the df is a training example when passed into the TextDataset\n",
        "    train_df_filtered = filter_short_texts(train_df, tokenizer, block_size)\n",
        "except FileNotFoundError:\n",
        "    train_df_filtered = None\n",
        "    print(f'Training data found in {os.getcwd()}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {},
      "outputs": [],
      "source": [
        "T = block_size\n",
        "B = 64\n",
        "train_dataset = TextDataset(train_df_filtered.text, tokenizer, block_size=T)\n",
        "train_loader = DataLoader(train_dataset, batch_size=B, shuffle=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UZnPMNE95IUs"
      },
      "source": [
        "## Model Finetuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [],
      "source": [
        "use_large = False\n",
        "use_medium = True\n",
        "\n",
        "if use_large:\n",
        "    pretrained_model = GPT2LMHeadModel.from_pretrained('gpt2-large')\n",
        "    model = BayesGPT(GPT2Config(n_layer=36, n_head=20, n_embd=1280))\n",
        "elif use_medium:\n",
        "    pretrained_model = GPT2LMHeadModel.from_pretrained('gpt2-medium')\n",
        "    model = BayesGPT(GPT2Config(n_layer=24, n_head=16, n_embd=1024))\n",
        "else:\n",
        "    pretrained_model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
        "    model = BayesGPT(GPT2Config())\n",
        "\n",
        "model.load_pretrained_model(pretrained_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "sSD-SJFurIpf"
      },
      "outputs": [],
      "source": [
        "torch.set_float32_matmul_precision('high')\n",
        "\n",
        "if ddp:\n",
        "    model = DDP(model, device_ids=[ddp_local_rank])\n",
        "raw_model = model.module if ddp else model\n",
        "\n",
        "weight_decay = 0.1\n",
        "learning_rate = 6e-4\n",
        "\n",
        "optimizer = raw_model.configure_optimizers(weight_decay, learning_rate, device_type)\n",
        "ce_criterion = nn.CrossEntropyLoss()\n",
        "kl_criterion = bnn.BKLLoss(reduction='mean', last_layer_only=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "Lmv40A34GWiX"
      },
      "outputs": [],
      "source": [
        "def get_lr(it: int, max_lr: float, min_lr: float, warmup_steps: int, max_steps: int) -> float:\n",
        "    if it < warmup_steps:\n",
        "        return max_lr * (it + 1) / warmup_steps\n",
        "    if it > max_steps:\n",
        "        return min_lr\n",
        "    decay_ratio = (it - warmup_steps) / (max_steps - warmup_steps)\n",
        "    assert 0 <= decay_ratio <= 1\n",
        "    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio))\n",
        "    return min_lr + coeff * (max_lr - min_lr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "log_dir = 'log'\n",
        "os.makedirs(log_dir, exist_ok=True)\n",
        "log_file = os.path.join(log_dir, 'log.txt')\n",
        "with open(log_file, 'w') as f:\n",
        "    pass\n",
        "\n",
        "kl_weight = 0.01\n",
        "max_lr = 6e-4\n",
        "min_lr = max_lr * 0.1\n",
        "warmup_steps = 0\n",
        "max_steps = 1000\n",
        "\n",
        "# grad_accum_steps = len(train_dataset) // (B * T * ddp_world_size)\n",
        "grad_accum_steps = 8\n",
        "\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TcbgeutRwYFD",
        "outputId": "e260e6ad-389b-4094-a360-1afbf08c9b8e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "step     0 | loss: 6.051754 | lr 6.0000e-04 | norm: 0.5479 | dt: 2926.50ms | tok/sec: 2799.25\n",
            "step     1 | loss: 6.042659 | lr 6.0000e-04 | norm: 0.5707 | dt: 2856.49ms | tok/sec: 2867.86\n",
            "step     2 | loss: 5.818207 | lr 5.9999e-04 | norm: 0.5865 | dt: 2885.74ms | tok/sec: 2838.78\n",
            "step     3 | loss: 5.938889 | lr 5.9999e-04 | norm: 0.6309 | dt: 2893.19ms | tok/sec: 2831.47\n",
            "step     4 | loss: 5.859262 | lr 5.9998e-04 | norm: 0.6302 | dt: 2903.23ms | tok/sec: 2821.68\n",
            "step     5 | loss: 6.100172 | lr 5.9997e-04 | norm: 0.6612 | dt: 2909.45ms | tok/sec: 2815.65\n",
            "step     6 | loss: 6.063196 | lr 5.9995e-04 | norm: 0.7159 | dt: 2906.29ms | tok/sec: 2818.71\n",
            "step     7 | loss: 5.983959 | lr 5.9993e-04 | norm: 0.6396 | dt: 2918.45ms | tok/sec: 2806.97\n",
            "step     8 | loss: 5.976865 | lr 5.9991e-04 | norm: 0.6391 | dt: 2922.14ms | tok/sec: 2803.43\n",
            "step     9 | loss: 5.862965 | lr 5.9989e-04 | norm: 0.6391 | dt: 2939.90ms | tok/sec: 2786.49\n",
            "step    10 | loss: 5.841560 | lr 5.9987e-04 | norm: 0.5843 | dt: 2937.30ms | tok/sec: 2788.96\n",
            "step    11 | loss: 5.933829 | lr 5.9984e-04 | norm: 0.6258 | dt: 2974.39ms | tok/sec: 2754.18\n",
            "step    12 | loss: 5.922621 | lr 5.9981e-04 | norm: 0.6627 | dt: 2982.92ms | tok/sec: 2746.30\n",
            "step    13 | loss: 5.943236 | lr 5.9977e-04 | norm: 0.7016 | dt: 3004.79ms | tok/sec: 2726.32\n",
            "step    14 | loss: 5.753436 | lr 5.9974e-04 | norm: 0.7416 | dt: 2991.57ms | tok/sec: 2738.36\n",
            "step    15 | loss: 5.831846 | lr 5.9970e-04 | norm: 0.6582 | dt: 3021.81ms | tok/sec: 2710.96\n",
            "step    16 | loss: 5.815270 | lr 5.9966e-04 | norm: 0.6919 | dt: 2987.71ms | tok/sec: 2741.90\n",
            "step    17 | loss: 5.739439 | lr 5.9962e-04 | norm: 0.7080 | dt: 3230.85ms | tok/sec: 2535.56\n",
            "step    18 | loss: 5.685878 | lr 5.9957e-04 | norm: 0.6591 | dt: 3265.89ms | tok/sec: 2508.35\n",
            "step    19 | loss: 5.935019 | lr 5.9952e-04 | norm: 0.7018 | dt: 2964.52ms | tok/sec: 2763.34\n",
            "step    20 | loss: 6.089619 | lr 5.9947e-04 | norm: 0.6948 | dt: 2937.72ms | tok/sec: 2788.56\n",
            "step    21 | loss: 5.767854 | lr 5.9941e-04 | norm: 0.6760 | dt: 2925.40ms | tok/sec: 2800.30\n",
            "step    22 | loss: 5.716321 | lr 5.9936e-04 | norm: 0.6537 | dt: 2911.68ms | tok/sec: 2813.50\n",
            "step    23 | loss: 5.827293 | lr 5.9930e-04 | norm: 0.6836 | dt: 2897.74ms | tok/sec: 2827.03\n",
            "step    24 | loss: 5.826891 | lr 5.9923e-04 | norm: 0.7714 | dt: 2898.72ms | tok/sec: 2826.08\n",
            "step    25 | loss: 5.887446 | lr 5.9917e-04 | norm: 0.6746 | dt: 2890.21ms | tok/sec: 2834.39\n",
            "step    26 | loss: 5.747910 | lr 5.9910e-04 | norm: 0.5783 | dt: 2899.78ms | tok/sec: 2825.05\n",
            "step    27 | loss: 5.717130 | lr 5.9903e-04 | norm: 0.6281 | dt: 2927.24ms | tok/sec: 2798.54\n",
            "step    28 | loss: 5.984658 | lr 5.9896e-04 | norm: 0.6598 | dt: 2873.32ms | tok/sec: 2851.05\n",
            "step    29 | loss: 5.812573 | lr 5.9888e-04 | norm: 0.6065 | dt: 2878.54ms | tok/sec: 2845.89\n",
            "step    30 | loss: 5.654271 | lr 5.9880e-04 | norm: 0.6432 | dt: 2892.75ms | tok/sec: 2831.91\n",
            "step    31 | loss: 5.715648 | lr 5.9872e-04 | norm: 0.7352 | dt: 2944.99ms | tok/sec: 2781.67\n",
            "step    32 | loss: 5.774696 | lr 5.9864e-04 | norm: 0.7515 | dt: 2887.13ms | tok/sec: 2837.42\n",
            "step    33 | loss: 5.635456 | lr 5.9855e-04 | norm: 0.5943 | dt: 2878.23ms | tok/sec: 2846.20\n",
            "step    34 | loss: 5.876667 | lr 5.9846e-04 | norm: 0.6245 | dt: 2884.93ms | tok/sec: 2839.58\n",
            "step    35 | loss: 5.698142 | lr 5.9837e-04 | norm: 0.6371 | dt: 2898.50ms | tok/sec: 2826.29\n",
            "step    36 | loss: 5.783258 | lr 5.9828e-04 | norm: 0.6047 | dt: 2876.64ms | tok/sec: 2847.76\n",
            "step    37 | loss: 5.798884 | lr 5.9818e-04 | norm: 0.6254 | dt: 2892.92ms | tok/sec: 2831.75\n",
            "step    38 | loss: 5.900783 | lr 5.9808e-04 | norm: 0.6504 | dt: 2904.96ms | tok/sec: 2820.01\n",
            "step    39 | loss: 5.683788 | lr 5.9798e-04 | norm: 0.6767 | dt: 2915.41ms | tok/sec: 2809.90\n",
            "step    40 | loss: 5.746117 | lr 5.9787e-04 | norm: 0.6286 | dt: 2927.43ms | tok/sec: 2798.36\n",
            "step    41 | loss: 5.487563 | lr 5.9776e-04 | norm: 0.5979 | dt: 2914.69ms | tok/sec: 2810.59\n",
            "step    42 | loss: 5.786674 | lr 5.9765e-04 | norm: 0.6402 | dt: 2930.75ms | tok/sec: 2795.19\n",
            "step    43 | loss: 5.703146 | lr 5.9754e-04 | norm: 0.6466 | dt: 2931.48ms | tok/sec: 2794.49\n",
            "step    44 | loss: 5.712246 | lr 5.9742e-04 | norm: 0.6587 | dt: 2950.16ms | tok/sec: 2776.80\n",
            "step    45 | loss: 5.734988 | lr 5.9731e-04 | norm: 0.6127 | dt: 2931.69ms | tok/sec: 2794.29\n",
            "step    46 | loss: 5.700181 | lr 5.9719e-04 | norm: 0.6094 | dt: 2935.93ms | tok/sec: 2790.26\n",
            "step    47 | loss: 5.720508 | lr 5.9706e-04 | norm: 0.6223 | dt: 2923.35ms | tok/sec: 2802.27\n",
            "step    48 | loss: 5.756215 | lr 5.9694e-04 | norm: 0.6353 | dt: 2946.58ms | tok/sec: 2780.17\n",
            "step    49 | loss: 5.634724 | lr 5.9681e-04 | norm: 0.6560 | dt: 2937.86ms | tok/sec: 2788.43\n",
            "step    50 | loss: 5.697686 | lr 5.9668e-04 | norm: 0.6076 | dt: 2923.18ms | tok/sec: 2802.43\n",
            "step    51 | loss: 5.652209 | lr 5.9654e-04 | norm: 0.6113 | dt: 2914.84ms | tok/sec: 2810.45\n",
            "step    52 | loss: 5.708692 | lr 5.9641e-04 | norm: 0.6302 | dt: 2930.80ms | tok/sec: 2795.15\n",
            "step    53 | loss: 5.686632 | lr 5.9627e-04 | norm: 0.6309 | dt: 2908.40ms | tok/sec: 2816.67\n",
            "step    54 | loss: 5.589145 | lr 5.9612e-04 | norm: 0.6151 | dt: 2979.12ms | tok/sec: 2749.81\n",
            "step    55 | loss: 5.532357 | lr 5.9598e-04 | norm: 0.6185 | dt: 2911.14ms | tok/sec: 2814.02\n",
            "step    56 | loss: 5.683521 | lr 5.9583e-04 | norm: 0.6272 | dt: 2944.29ms | tok/sec: 2782.34\n",
            "step    57 | loss: 5.412662 | lr 5.9568e-04 | norm: 0.5866 | dt: 2907.89ms | tok/sec: 2817.16\n",
            "step    58 | loss: 5.493133 | lr 5.9553e-04 | norm: 0.6100 | dt: 2892.14ms | tok/sec: 2832.51\n",
            "step    59 | loss: 5.591808 | lr 5.9538e-04 | norm: 0.6117 | dt: 2891.10ms | tok/sec: 2833.53\n",
            "step    60 | loss: 5.629408 | lr 5.9522e-04 | norm: 0.6633 | dt: 2895.56ms | tok/sec: 2829.15\n",
            "step    61 | loss: 5.437618 | lr 5.9506e-04 | norm: 0.6131 | dt: 2910.60ms | tok/sec: 2814.54\n",
            "step    62 | loss: 5.494947 | lr 5.9489e-04 | norm: 0.5731 | dt: 2889.11ms | tok/sec: 2835.47\n",
            "step    63 | loss: 5.488219 | lr 5.9473e-04 | norm: 0.6238 | dt: 2902.32ms | tok/sec: 2822.57\n",
            "step    64 | loss: 5.499976 | lr 5.9456e-04 | norm: 0.6559 | dt: 2902.05ms | tok/sec: 2822.84\n",
            "step    65 | loss: 5.431868 | lr 5.9439e-04 | norm: 0.6133 | dt: 2925.13ms | tok/sec: 2800.56\n",
            "step    66 | loss: 5.673709 | lr 5.9422e-04 | norm: 0.7023 | dt: 2900.08ms | tok/sec: 2824.75\n",
            "step    67 | loss: 5.655586 | lr 5.9404e-04 | norm: 0.6844 | dt: 2921.10ms | tok/sec: 2804.42\n",
            "step    68 | loss: 5.505188 | lr 5.9386e-04 | norm: 0.6263 | dt: 2912.58ms | tok/sec: 2812.63\n",
            "step    69 | loss: 5.638954 | lr 5.9368e-04 | norm: 0.6644 | dt: 2928.40ms | tok/sec: 2797.43\n",
            "step    70 | loss: 5.469517 | lr 5.9350e-04 | norm: 0.5708 | dt: 2909.93ms | tok/sec: 2815.19\n",
            "step    71 | loss: 5.567070 | lr 5.9331e-04 | norm: 0.6634 | dt: 2903.51ms | tok/sec: 2821.41\n",
            "step    72 | loss: 5.585535 | lr 5.9312e-04 | norm: 0.6472 | dt: 2909.76ms | tok/sec: 2815.35\n",
            "step    73 | loss: 5.512599 | lr 5.9293e-04 | norm: 0.6066 | dt: 2933.84ms | tok/sec: 2792.24\n",
            "step    74 | loss: 5.453062 | lr 5.9274e-04 | norm: 0.5839 | dt: 2910.42ms | tok/sec: 2814.72\n",
            "step    75 | loss: 5.402894 | lr 5.9254e-04 | norm: 0.6290 | dt: 2910.15ms | tok/sec: 2814.98\n",
            "step    76 | loss: 5.752466 | lr 5.9234e-04 | norm: 0.6535 | dt: 2909.55ms | tok/sec: 2815.55\n",
            "step    77 | loss: 5.519454 | lr 5.9214e-04 | norm: 0.6078 | dt: 2922.87ms | tok/sec: 2802.72\n",
            "step    78 | loss: 5.376468 | lr 5.9193e-04 | norm: 0.6035 | dt: 2917.58ms | tok/sec: 2807.81\n",
            "step    79 | loss: 5.288353 | lr 5.9173e-04 | norm: 0.5902 | dt: 2921.91ms | tok/sec: 2803.64\n",
            "step    80 | loss: 5.280225 | lr 5.9152e-04 | norm: 0.5846 | dt: 2914.19ms | tok/sec: 2811.08\n",
            "step    81 | loss: 5.450480 | lr 5.9131e-04 | norm: 0.6264 | dt: 2929.31ms | tok/sec: 2796.57\n",
            "step    82 | loss: 5.304043 | lr 5.9109e-04 | norm: 0.6581 | dt: 2921.79ms | tok/sec: 2803.77\n",
            "step    83 | loss: 5.310822 | lr 5.9087e-04 | norm: 0.6494 | dt: 2912.33ms | tok/sec: 2812.87\n",
            "step    84 | loss: 5.447367 | lr 5.9065e-04 | norm: 0.9759 | dt: 2916.09ms | tok/sec: 2809.24\n",
            "step    85 | loss: 5.452287 | lr 5.9043e-04 | norm: 0.6123 | dt: 2929.68ms | tok/sec: 2796.21\n",
            "step    86 | loss: 5.440876 | lr 5.9021e-04 | norm: 0.5809 | dt: 2925.49ms | tok/sec: 2800.22\n",
            "step    87 | loss: 5.396829 | lr 5.8998e-04 | norm: 0.6164 | dt: 2927.88ms | tok/sec: 2797.93\n",
            "step    88 | loss: 5.355516 | lr 5.8975e-04 | norm: 0.5780 | dt: 2976.73ms | tok/sec: 2752.01\n",
            "step    89 | loss: 5.499745 | lr 5.8951e-04 | norm: 0.7006 | dt: 2907.91ms | tok/sec: 2817.15\n",
            "step    90 | loss: 5.305339 | lr 5.8928e-04 | norm: 0.6305 | dt: 2944.12ms | tok/sec: 2782.50\n",
            "step    91 | loss: 5.449999 | lr 5.8904e-04 | norm: 0.7134 | dt: 2912.03ms | tok/sec: 2813.16\n",
            "step    92 | loss: 5.315201 | lr 5.8880e-04 | norm: 0.6059 | dt: 2923.33ms | tok/sec: 2802.28\n",
            "step    93 | loss: 5.417141 | lr 5.8856e-04 | norm: 0.5918 | dt: 2907.33ms | tok/sec: 2817.70\n",
            "step    94 | loss: 5.304924 | lr 5.8831e-04 | norm: 0.6787 | dt: 2912.05ms | tok/sec: 2813.14\n",
            "step    95 | loss: 5.249830 | lr 5.8806e-04 | norm: 0.6045 | dt: 2918.60ms | tok/sec: 2806.82\n",
            "step    96 | loss: 5.459816 | lr 5.8781e-04 | norm: 0.6497 | dt: 2918.66ms | tok/sec: 2806.77\n",
            "step    97 | loss: 5.360261 | lr 5.8756e-04 | norm: 0.6207 | dt: 2904.87ms | tok/sec: 2820.09\n",
            "step    98 | loss: 5.372829 | lr 5.8730e-04 | norm: 0.6615 | dt: 2917.47ms | tok/sec: 2807.92\n",
            "step    99 | loss: 5.465590 | lr 5.8705e-04 | norm: 0.6440 | dt: 2934.18ms | tok/sec: 2791.92\n",
            "step   100 | loss: 5.215031 | lr 5.8679e-04 | norm: 0.5884 | dt: 2906.07ms | tok/sec: 2818.93\n",
            "step   101 | loss: 5.487553 | lr 5.8652e-04 | norm: 0.6089 | dt: 2901.77ms | tok/sec: 2823.11\n",
            "step   102 | loss: 5.174416 | lr 5.8626e-04 | norm: 0.6209 | dt: 2915.79ms | tok/sec: 2809.53\n",
            "step   103 | loss: 5.452422 | lr 5.8599e-04 | norm: 0.6194 | dt: 2920.32ms | tok/sec: 2805.17\n",
            "step   104 | loss: 5.176311 | lr 5.8572e-04 | norm: 0.5840 | dt: 2912.71ms | tok/sec: 2812.50\n",
            "step   105 | loss: 5.321088 | lr 5.8544e-04 | norm: 0.6322 | dt: 2911.86ms | tok/sec: 2813.32\n",
            "step   106 | loss: 5.368477 | lr 5.8517e-04 | norm: 0.6426 | dt: 2906.79ms | tok/sec: 2818.23\n",
            "step   107 | loss: 5.339163 | lr 5.8489e-04 | norm: 0.6571 | dt: 2911.97ms | tok/sec: 2813.22\n",
            "step   108 | loss: 5.127726 | lr 5.8461e-04 | norm: 0.6193 | dt: 2914.86ms | tok/sec: 2810.43\n",
            "step   109 | loss: 5.113363 | lr 5.8432e-04 | norm: 0.5692 | dt: 2900.63ms | tok/sec: 2824.21\n",
            "step   110 | loss: 5.293438 | lr 5.8404e-04 | norm: 0.6252 | dt: 2906.13ms | tok/sec: 2818.87\n",
            "step   111 | loss: 5.396918 | lr 5.8375e-04 | norm: 0.6813 | dt: 2912.63ms | tok/sec: 2812.58\n",
            "step   112 | loss: 5.317698 | lr 5.8346e-04 | norm: 0.6499 | dt: 2907.99ms | tok/sec: 2817.06\n",
            "step   113 | loss: 4.975071 | lr 5.8316e-04 | norm: 0.6264 | dt: 2904.37ms | tok/sec: 2820.58\n",
            "step   114 | loss: 5.149947 | lr 5.8287e-04 | norm: 0.7310 | dt: 2901.92ms | tok/sec: 2822.96\n",
            "step   115 | loss: 5.148058 | lr 5.8257e-04 | norm: 0.5931 | dt: 2910.40ms | tok/sec: 2814.73\n",
            "step   116 | loss: 5.268819 | lr 5.8227e-04 | norm: 0.6676 | dt: 2936.93ms | tok/sec: 2789.31\n",
            "step   117 | loss: 5.106894 | lr 5.8197e-04 | norm: 0.5985 | dt: 2909.49ms | tok/sec: 2815.61\n",
            "step   118 | loss: 5.062865 | lr 5.8166e-04 | norm: 0.6483 | dt: 2905.25ms | tok/sec: 2819.72\n",
            "step   119 | loss: 5.195723 | lr 5.8135e-04 | norm: 0.6286 | dt: 2904.40ms | tok/sec: 2820.55\n",
            "step   120 | loss: 5.051133 | lr 5.8104e-04 | norm: 0.6064 | dt: 2931.39ms | tok/sec: 2794.58\n",
            "step   121 | loss: 5.287590 | lr 5.8073e-04 | norm: 0.6793 | dt: 2907.51ms | tok/sec: 2817.53\n",
            "step   122 | loss: 5.128147 | lr 5.8041e-04 | norm: 0.6094 | dt: 2902.08ms | tok/sec: 2822.80\n",
            "step   123 | loss: 5.246611 | lr 5.8009e-04 | norm: 0.6325 | dt: 2901.62ms | tok/sec: 2823.25\n",
            "step   124 | loss: 5.192318 | lr 5.7977e-04 | norm: 0.6061 | dt: 2928.66ms | tok/sec: 2797.18\n",
            "step   125 | loss: 5.114390 | lr 5.7945e-04 | norm: 0.6466 | dt: 2892.95ms | tok/sec: 2831.71\n",
            "step   126 | loss: 4.957462 | lr 5.7912e-04 | norm: 0.6304 | dt: 2902.56ms | tok/sec: 2822.34\n",
            "step   127 | loss: 5.212030 | lr 5.7879e-04 | norm: 0.6494 | dt: 2902.31ms | tok/sec: 2822.57\n",
            "step   128 | loss: 5.045984 | lr 5.7846e-04 | norm: 0.6326 | dt: 2901.94ms | tok/sec: 2822.94\n",
            "step   129 | loss: 5.282289 | lr 5.7813e-04 | norm: 0.7189 | dt: 2901.44ms | tok/sec: 2823.43\n",
            "step   130 | loss: 5.024122 | lr 5.7779e-04 | norm: 0.5844 | dt: 2903.62ms | tok/sec: 2821.30\n",
            "step   131 | loss: 5.258703 | lr 5.7746e-04 | norm: 0.6276 | dt: 2903.40ms | tok/sec: 2821.52\n",
            "step   132 | loss: 5.105334 | lr 5.7712e-04 | norm: 0.6007 | dt: 2901.61ms | tok/sec: 2823.26\n",
            "step   133 | loss: 5.149353 | lr 5.7677e-04 | norm: 0.6253 | dt: 2924.09ms | tok/sec: 2801.56\n",
            "step   134 | loss: 5.246944 | lr 5.7643e-04 | norm: 0.6266 | dt: 2910.09ms | tok/sec: 2815.03\n",
            "step   135 | loss: 5.143018 | lr 5.7608e-04 | norm: 0.6255 | dt: 2900.63ms | tok/sec: 2824.21\n",
            "step   136 | loss: 5.110174 | lr 5.7573e-04 | norm: 0.6432 | dt: 2914.28ms | tok/sec: 2810.99\n",
            "step   137 | loss: 5.240767 | lr 5.7538e-04 | norm: 0.6470 | dt: 2934.56ms | tok/sec: 2791.56\n",
            "step   138 | loss: 5.038466 | lr 5.7502e-04 | norm: 0.6539 | dt: 2906.61ms | tok/sec: 2818.41\n",
            "step   139 | loss: 4.960948 | lr 5.7466e-04 | norm: 0.6136 | dt: 2903.81ms | tok/sec: 2821.12\n",
            "step   140 | loss: 5.099796 | lr 5.7430e-04 | norm: 0.6375 | dt: 2911.15ms | tok/sec: 2814.01\n",
            "step   141 | loss: 5.199683 | lr 5.7394e-04 | norm: 0.6669 | dt: 2929.30ms | tok/sec: 2796.57\n",
            "step   142 | loss: 5.026666 | lr 5.7358e-04 | norm: 0.6490 | dt: 2911.00ms | tok/sec: 2814.16\n",
            "step   143 | loss: 5.124476 | lr 5.7321e-04 | norm: 0.6611 | dt: 2922.92ms | tok/sec: 2802.68\n",
            "step   144 | loss: 4.994203 | lr 5.7284e-04 | norm: 0.6066 | dt: 2909.40ms | tok/sec: 2815.70\n",
            "step   145 | loss: 4.922466 | lr 5.7247e-04 | norm: 0.6320 | dt: 2912.46ms | tok/sec: 2812.74\n",
            "step   146 | loss: 5.135568 | lr 5.7209e-04 | norm: 0.6240 | dt: 2909.24ms | tok/sec: 2815.86\n",
            "step   147 | loss: 5.018909 | lr 5.7172e-04 | norm: 0.6758 | dt: 2909.65ms | tok/sec: 2815.46\n",
            "step   148 | loss: 4.982235 | lr 5.7134e-04 | norm: 0.6836 | dt: 2906.89ms | tok/sec: 2818.13\n",
            "step   149 | loss: 4.859992 | lr 5.7096e-04 | norm: 1.1742 | dt: 2908.61ms | tok/sec: 2816.47\n",
            "step   150 | loss: 5.073665 | lr 5.7057e-04 | norm: 0.6786 | dt: 2904.28ms | tok/sec: 2820.66\n",
            "step   151 | loss: 5.131209 | lr 5.7019e-04 | norm: 0.7741 | dt: 2903.23ms | tok/sec: 2821.68\n",
            "step   152 | loss: 5.094320 | lr 5.6980e-04 | norm: 0.6479 | dt: 2910.70ms | tok/sec: 2814.45\n",
            "step   153 | loss: 5.243040 | lr 5.6941e-04 | norm: 0.6815 | dt: 2909.70ms | tok/sec: 2815.41\n",
            "step   154 | loss: 5.294648 | lr 5.6901e-04 | norm: 0.6533 | dt: 2949.65ms | tok/sec: 2777.27\n",
            "step   155 | loss: 5.118974 | lr 5.6862e-04 | norm: 0.6163 | dt: 2911.38ms | tok/sec: 2813.78\n",
            "step   156 | loss: 4.918461 | lr 5.6822e-04 | norm: 0.5956 | dt: 2906.46ms | tok/sec: 2818.55\n",
            "step   157 | loss: 4.934942 | lr 5.6782e-04 | norm: 0.5911 | dt: 2913.57ms | tok/sec: 2811.67\n",
            "step   158 | loss: 4.999642 | lr 5.6742e-04 | norm: 0.5804 | dt: 2905.31ms | tok/sec: 2819.67\n",
            "step   159 | loss: 4.825368 | lr 5.6701e-04 | norm: 0.6300 | dt: 2906.06ms | tok/sec: 2818.94\n",
            "step   160 | loss: 5.022643 | lr 5.6660e-04 | norm: 0.7587 | dt: 2892.67ms | tok/sec: 2831.98\n",
            "step   161 | loss: 4.878060 | lr 5.6619e-04 | norm: 0.6432 | dt: 2900.95ms | tok/sec: 2823.91\n",
            "step   162 | loss: 4.864478 | lr 5.6578e-04 | norm: 0.6193 | dt: 2917.23ms | tok/sec: 2808.14\n",
            "step   163 | loss: 4.938000 | lr 5.6537e-04 | norm: 0.6268 | dt: 2911.18ms | tok/sec: 2813.98\n",
            "step   164 | loss: 5.051167 | lr 5.6495e-04 | norm: 0.6698 | dt: 2907.88ms | tok/sec: 2817.17\n",
            "step   165 | loss: 4.811676 | lr 5.6453e-04 | norm: 0.6735 | dt: 2908.80ms | tok/sec: 2816.28\n",
            "step   166 | loss: 4.904731 | lr 5.6411e-04 | norm: 0.6522 | dt: 2894.14ms | tok/sec: 2830.54\n",
            "step   167 | loss: 5.073566 | lr 5.6369e-04 | norm: 0.6358 | dt: 2925.59ms | tok/sec: 2800.11\n",
            "step   168 | loss: 5.133018 | lr 5.6326e-04 | norm: 0.6520 | dt: 2913.00ms | tok/sec: 2812.22\n",
            "step   169 | loss: 5.103449 | lr 5.6283e-04 | norm: 0.6532 | dt: 2910.56ms | tok/sec: 2814.58\n",
            "step   170 | loss: 5.003815 | lr 5.6240e-04 | norm: 0.6618 | dt: 2901.17ms | tok/sec: 2823.68\n",
            "step   171 | loss: 4.872157 | lr 5.6197e-04 | norm: 0.6255 | dt: 2920.07ms | tok/sec: 2805.41\n",
            "step   172 | loss: 5.006663 | lr 5.6153e-04 | norm: 0.6102 | dt: 2895.18ms | tok/sec: 2829.53\n",
            "step   173 | loss: 4.749847 | lr 5.6109e-04 | norm: 0.6554 | dt: 2912.48ms | tok/sec: 2812.72\n",
            "step   174 | loss: 4.646777 | lr 5.6065e-04 | norm: 0.6302 | dt: 2907.63ms | tok/sec: 2817.42\n",
            "step   175 | loss: 4.895773 | lr 5.6021e-04 | norm: 0.6780 | dt: 2920.82ms | tok/sec: 2804.69\n",
            "step   176 | loss: 4.936076 | lr 5.5977e-04 | norm: 0.6789 | dt: 2910.68ms | tok/sec: 2814.46\n",
            "step   177 | loss: 4.857226 | lr 5.5932e-04 | norm: 0.7309 | dt: 2909.23ms | tok/sec: 2815.87\n",
            "step   178 | loss: 4.836642 | lr 5.5887e-04 | norm: 0.6645 | dt: 2908.07ms | tok/sec: 2816.99\n",
            "step   179 | loss: 4.868726 | lr 5.5842e-04 | norm: 0.6393 | dt: 2918.03ms | tok/sec: 2807.37\n",
            "step   180 | loss: 4.916338 | lr 5.5797e-04 | norm: 0.6315 | dt: 2920.35ms | tok/sec: 2805.14\n",
            "step   181 | loss: 4.747581 | lr 5.5751e-04 | norm: 0.6397 | dt: 2906.03ms | tok/sec: 2818.96\n",
            "step   182 | loss: 4.789663 | lr 5.5706e-04 | norm: 0.6890 | dt: 2907.64ms | tok/sec: 2817.40\n",
            "step   183 | loss: 4.868897 | lr 5.5659e-04 | norm: 0.6681 | dt: 2906.42ms | tok/sec: 2818.59\n",
            "step   184 | loss: 4.796038 | lr 5.5613e-04 | norm: 0.6601 | dt: 2934.83ms | tok/sec: 2791.31\n",
            "step   185 | loss: 4.980214 | lr 5.5567e-04 | norm: 0.6798 | dt: 2908.97ms | tok/sec: 2816.12\n",
            "step   186 | loss: 4.844194 | lr 5.5520e-04 | norm: 0.6628 | dt: 2908.03ms | tok/sec: 2817.03\n",
            "step   187 | loss: 4.778496 | lr 5.5473e-04 | norm: 0.6056 | dt: 2925.08ms | tok/sec: 2800.61\n",
            "step   188 | loss: 4.864359 | lr 5.5426e-04 | norm: 0.6133 | dt: 2932.85ms | tok/sec: 2793.18\n",
            "step   189 | loss: 4.920402 | lr 5.5379e-04 | norm: 0.6804 | dt: 2913.77ms | tok/sec: 2811.48\n",
            "step   190 | loss: 4.663571 | lr 5.5331e-04 | norm: 0.6583 | dt: 2907.24ms | tok/sec: 2817.79\n",
            "step   191 | loss: 4.862425 | lr 5.5283e-04 | norm: 0.7235 | dt: 2922.46ms | tok/sec: 2803.12\n",
            "step   192 | loss: 4.889691 | lr 5.5235e-04 | norm: 0.7036 | dt: 2958.92ms | tok/sec: 2768.57\n",
            "step   193 | loss: 4.906763 | lr 5.5187e-04 | norm: 0.7256 | dt: 2914.49ms | tok/sec: 2810.78\n",
            "step   194 | loss: 4.831545 | lr 5.5139e-04 | norm: 0.6620 | dt: 2909.34ms | tok/sec: 2815.76\n",
            "step   195 | loss: 4.671165 | lr 5.5090e-04 | norm: 0.6071 | dt: 2906.38ms | tok/sec: 2818.62\n",
            "step   196 | loss: 4.640605 | lr 5.5041e-04 | norm: 0.6267 | dt: 2930.49ms | tok/sec: 2795.43\n",
            "step   197 | loss: 4.734478 | lr 5.4992e-04 | norm: 0.6548 | dt: 2909.11ms | tok/sec: 2815.99\n",
            "step   198 | loss: 4.756660 | lr 5.4943e-04 | norm: 0.7086 | dt: 2917.40ms | tok/sec: 2807.98\n",
            "step   199 | loss: 4.788512 | lr 5.4893e-04 | norm: 0.7076 | dt: 2912.54ms | tok/sec: 2812.67\n",
            "step   200 | loss: 4.857290 | lr 5.4843e-04 | norm: 0.7171 | dt: 2908.91ms | tok/sec: 2816.17\n",
            "step   201 | loss: 4.768018 | lr 5.4793e-04 | norm: 0.6642 | dt: 2932.44ms | tok/sec: 2793.58\n",
            "step   202 | loss: 4.814498 | lr 5.4743e-04 | norm: 0.6857 | dt: 2896.78ms | tok/sec: 2827.97\n",
            "step   203 | loss: 4.725626 | lr 5.4693e-04 | norm: 0.6521 | dt: 2920.17ms | tok/sec: 2805.31\n",
            "step   204 | loss: 4.716600 | lr 5.4642e-04 | norm: 0.6430 | dt: 2910.48ms | tok/sec: 2814.66\n",
            "step   205 | loss: 4.723337 | lr 5.4591e-04 | norm: 0.6468 | dt: 2921.08ms | tok/sec: 2804.44\n",
            "step   206 | loss: 4.753093 | lr 5.4540e-04 | norm: 0.6223 | dt: 2904.05ms | tok/sec: 2820.89\n",
            "step   207 | loss: 4.669683 | lr 5.4489e-04 | norm: 0.6390 | dt: 2908.27ms | tok/sec: 2816.80\n",
            "step   208 | loss: 4.671352 | lr 5.4438e-04 | norm: 0.6318 | dt: 2913.89ms | tok/sec: 2811.36\n",
            "step   209 | loss: 4.757272 | lr 5.4386e-04 | norm: 0.7021 | dt: 2923.81ms | tok/sec: 2801.82\n",
            "step   210 | loss: 4.612434 | lr 5.4334e-04 | norm: 0.6435 | dt: 2901.15ms | tok/sec: 2823.71\n",
            "step   211 | loss: 4.907518 | lr 5.4282e-04 | norm: 0.6701 | dt: 2902.78ms | tok/sec: 2822.12\n",
            "step   212 | loss: 4.725018 | lr 5.4230e-04 | norm: 0.6624 | dt: 2910.16ms | tok/sec: 2814.96\n",
            "step   213 | loss: 4.743234 | lr 5.4177e-04 | norm: 0.6556 | dt: 2909.13ms | tok/sec: 2815.96\n",
            "step   214 | loss: 4.854384 | lr 5.4125e-04 | norm: 0.6747 | dt: 2913.97ms | tok/sec: 2811.28\n",
            "step   215 | loss: 4.756902 | lr 5.4072e-04 | norm: 0.6458 | dt: 2898.85ms | tok/sec: 2825.95\n",
            "step   216 | loss: 4.676782 | lr 5.4018e-04 | norm: 0.6217 | dt: 2916.74ms | tok/sec: 2808.61\n",
            "step   217 | loss: 4.653903 | lr 5.3965e-04 | norm: 0.6520 | dt: 2911.94ms | tok/sec: 2813.24\n",
            "step   218 | loss: 4.755441 | lr 5.3912e-04 | norm: 0.6560 | dt: 2915.69ms | tok/sec: 2809.62\n",
            "step   219 | loss: 4.562339 | lr 5.3858e-04 | norm: 0.6515 | dt: 2900.31ms | tok/sec: 2824.52\n",
            "step   220 | loss: 4.555072 | lr 5.3804e-04 | norm: 0.6358 | dt: 2902.76ms | tok/sec: 2822.14\n",
            "step   221 | loss: 4.717875 | lr 5.3750e-04 | norm: 0.7433 | dt: 2907.84ms | tok/sec: 2817.22\n",
            "step   222 | loss: 4.876301 | lr 5.3695e-04 | norm: 0.7500 | dt: 2938.60ms | tok/sec: 2787.72\n",
            "step   223 | loss: 4.276710 | lr 5.3641e-04 | norm: 0.6167 | dt: 2911.16ms | tok/sec: 2814.00\n",
            "step   224 | loss: 4.560360 | lr 5.3586e-04 | norm: 0.6552 | dt: 2912.29ms | tok/sec: 2812.91\n",
            "step   225 | loss: 4.503832 | lr 5.3531e-04 | norm: 0.6415 | dt: 2909.31ms | tok/sec: 2815.79\n",
            "step   226 | loss: 4.880978 | lr 5.3476e-04 | norm: 0.7080 | dt: 2937.47ms | tok/sec: 2788.80\n",
            "step   227 | loss: 4.724982 | lr 5.3420e-04 | norm: 0.6475 | dt: 2904.68ms | tok/sec: 2820.28\n",
            "step   228 | loss: 4.735225 | lr 5.3365e-04 | norm: 0.6612 | dt: 2917.41ms | tok/sec: 2807.97\n",
            "step   229 | loss: 4.661920 | lr 5.3309e-04 | norm: 0.6283 | dt: 2905.42ms | tok/sec: 2819.56\n",
            "step   230 | loss: 4.738529 | lr 5.3253e-04 | norm: 0.6583 | dt: 2920.73ms | tok/sec: 2804.77\n",
            "step   231 | loss: 4.655617 | lr 5.3197e-04 | norm: 0.6644 | dt: 2918.64ms | tok/sec: 2806.78\n",
            "step   232 | loss: 4.513469 | lr 5.3140e-04 | norm: 0.6534 | dt: 2912.16ms | tok/sec: 2813.03\n",
            "step   233 | loss: 4.590199 | lr 5.3084e-04 | norm: 0.6502 | dt: 2911.12ms | tok/sec: 2814.03\n",
            "step   234 | loss: 4.652966 | lr 5.3027e-04 | norm: 0.6938 | dt: 2906.29ms | tok/sec: 2818.71\n",
            "step   235 | loss: 4.502066 | lr 5.2970e-04 | norm: 0.6942 | dt: 2938.32ms | tok/sec: 2787.99\n",
            "step   236 | loss: 4.468870 | lr 5.2913e-04 | norm: 0.6774 | dt: 2914.31ms | tok/sec: 2810.96\n",
            "step   237 | loss: 4.640303 | lr 5.2855e-04 | norm: 0.6364 | dt: 2916.90ms | tok/sec: 2808.46\n",
            "step   238 | loss: 4.695742 | lr 5.2798e-04 | norm: 0.7137 | dt: 2912.59ms | tok/sec: 2812.62\n",
            "step   239 | loss: 4.660295 | lr 5.2740e-04 | norm: 0.7128 | dt: 2923.54ms | tok/sec: 2802.08\n",
            "step   240 | loss: 4.635482 | lr 5.2682e-04 | norm: 0.7232 | dt: 2913.28ms | tok/sec: 2811.95\n",
            "step   241 | loss: 4.553632 | lr 5.2624e-04 | norm: 0.6844 | dt: 2914.43ms | tok/sec: 2810.84\n",
            "step   242 | loss: 4.499614 | lr 5.2566e-04 | norm: 0.6397 | dt: 2916.31ms | tok/sec: 2809.03\n",
            "step   243 | loss: 4.662690 | lr 5.2507e-04 | norm: 0.6707 | dt: 2930.74ms | tok/sec: 2795.20\n",
            "step   244 | loss: 4.316073 | lr 5.2448e-04 | norm: 0.6477 | dt: 2904.40ms | tok/sec: 2820.55\n",
            "step   245 | loss: 4.386796 | lr 5.2389e-04 | norm: 0.6332 | dt: 2905.47ms | tok/sec: 2819.51\n",
            "step   246 | loss: 4.409338 | lr 5.2330e-04 | norm: 0.6246 | dt: 2912.26ms | tok/sec: 2812.94\n",
            "step   247 | loss: 4.584717 | lr 5.2271e-04 | norm: 0.7083 | dt: 2934.04ms | tok/sec: 2792.06\n",
            "step   248 | loss: 4.577729 | lr 5.2211e-04 | norm: 0.7247 | dt: 2925.29ms | tok/sec: 2800.41\n",
            "step   249 | loss: 4.706628 | lr 5.2152e-04 | norm: 0.7689 | dt: 2913.54ms | tok/sec: 2811.70\n",
            "step   250 | loss: 4.445536 | lr 5.2092e-04 | norm: 0.6623 | dt: 13341.63ms | tok/sec: 614.02\n",
            "step   251 | loss: 4.528768 | lr 5.2032e-04 | norm: 0.6819 | dt: 2867.28ms | tok/sec: 2857.06\n",
            "step   252 | loss: 4.399941 | lr 5.1972e-04 | norm: 0.7452 | dt: 2899.20ms | tok/sec: 2825.61\n",
            "step   253 | loss: 4.656241 | lr 5.1911e-04 | norm: 0.6804 | dt: 2901.85ms | tok/sec: 2823.03\n",
            "step   254 | loss: 4.585954 | lr 5.1850e-04 | norm: 0.8752 | dt: 2915.61ms | tok/sec: 2809.70\n",
            "step   255 | loss: 4.556119 | lr 5.1790e-04 | norm: 0.7129 | dt: 2928.30ms | tok/sec: 2797.53\n",
            "step   256 | loss: 4.581699 | lr 5.1729e-04 | norm: 0.6945 | dt: 2963.17ms | tok/sec: 2764.61\n",
            "step   257 | loss: 4.376325 | lr 5.1667e-04 | norm: 0.6665 | dt: 2979.62ms | tok/sec: 2749.34\n",
            "step   258 | loss: 4.600940 | lr 5.1606e-04 | norm: 0.6899 | dt: 2991.76ms | tok/sec: 2738.19\n",
            "step   259 | loss: 4.466869 | lr 5.1545e-04 | norm: 0.6908 | dt: 2997.14ms | tok/sec: 2733.27\n",
            "step   260 | loss: 4.525938 | lr 5.1483e-04 | norm: 0.6867 | dt: 2993.22ms | tok/sec: 2736.85\n",
            "step   261 | loss: 4.396898 | lr 5.1421e-04 | norm: 0.6608 | dt: 2977.54ms | tok/sec: 2751.27\n",
            "step   262 | loss: 4.559952 | lr 5.1359e-04 | norm: 0.7199 | dt: 2953.85ms | tok/sec: 2773.33\n",
            "step   263 | loss: 4.702246 | lr 5.1296e-04 | norm: 0.7402 | dt: 2936.05ms | tok/sec: 2790.14\n",
            "step   264 | loss: 4.524066 | lr 5.1234e-04 | norm: 0.6844 | dt: 2937.98ms | tok/sec: 2788.31\n",
            "step   265 | loss: 4.528934 | lr 5.1171e-04 | norm: 0.6580 | dt: 2962.41ms | tok/sec: 2765.32\n",
            "step   266 | loss: 4.505885 | lr 5.1109e-04 | norm: 0.7107 | dt: 2909.96ms | tok/sec: 2815.16\n",
            "step   267 | loss: 4.416633 | lr 5.1046e-04 | norm: 0.6772 | dt: 2910.98ms | tok/sec: 2814.17\n",
            "step   268 | loss: 4.393251 | lr 5.0982e-04 | norm: 0.6697 | dt: 2901.15ms | tok/sec: 2823.71\n",
            "step   269 | loss: 4.445848 | lr 5.0919e-04 | norm: 0.7095 | dt: 2918.24ms | tok/sec: 2807.17\n",
            "step   270 | loss: 4.480553 | lr 5.0855e-04 | norm: 0.7068 | dt: 2882.34ms | tok/sec: 2842.13\n",
            "step   271 | loss: 4.519805 | lr 5.0792e-04 | norm: 0.6834 | dt: 2897.09ms | tok/sec: 2827.66\n",
            "step   272 | loss: 4.533988 | lr 5.0728e-04 | norm: 0.6847 | dt: 2887.23ms | tok/sec: 2837.32\n",
            "step   273 | loss: 4.556233 | lr 5.0664e-04 | norm: 0.6687 | dt: 2906.24ms | tok/sec: 2818.76\n",
            "step   274 | loss: 4.648330 | lr 5.0600e-04 | norm: 0.7041 | dt: 2874.28ms | tok/sec: 2850.11\n",
            "step   275 | loss: 4.267213 | lr 5.0535e-04 | norm: 0.6733 | dt: 2884.56ms | tok/sec: 2839.94\n",
            "step   276 | loss: 4.458579 | lr 5.0471e-04 | norm: 0.6539 | dt: 2890.79ms | tok/sec: 2833.83\n",
            "step   277 | loss: 4.293150 | lr 5.0406e-04 | norm: 0.6422 | dt: 2894.70ms | tok/sec: 2830.00\n",
            "step   278 | loss: 4.362917 | lr 5.0341e-04 | norm: 0.7115 | dt: 2895.67ms | tok/sec: 2829.05\n",
            "step   279 | loss: 4.223660 | lr 5.0276e-04 | norm: 0.7091 | dt: 2893.69ms | tok/sec: 2830.99\n",
            "step   280 | loss: 4.462551 | lr 5.0210e-04 | norm: 0.8374 | dt: 2905.76ms | tok/sec: 2819.22\n",
            "step   281 | loss: 4.251945 | lr 5.0145e-04 | norm: 0.6795 | dt: 2900.08ms | tok/sec: 2824.75\n",
            "step   282 | loss: 4.565531 | lr 5.0079e-04 | norm: 0.7312 | dt: 2928.11ms | tok/sec: 2797.70\n",
            "step   283 | loss: 4.423091 | lr 5.0014e-04 | norm: 0.6990 | dt: 2910.50ms | tok/sec: 2814.64\n",
            "step   284 | loss: 4.368373 | lr 4.9948e-04 | norm: 0.6934 | dt: 2910.47ms | tok/sec: 2814.67\n",
            "step   285 | loss: 4.354445 | lr 4.9882e-04 | norm: 0.6638 | dt: 2917.76ms | tok/sec: 2807.63\n",
            "step   286 | loss: 4.461000 | lr 4.9815e-04 | norm: 0.6689 | dt: 2933.65ms | tok/sec: 2792.42\n",
            "step   287 | loss: 4.165578 | lr 4.9749e-04 | norm: 0.6374 | dt: 2918.46ms | tok/sec: 2806.96\n",
            "step   288 | loss: 4.376270 | lr 4.9682e-04 | norm: 0.7052 | dt: 2924.97ms | tok/sec: 2800.71\n",
            "step   289 | loss: 4.380525 | lr 4.9615e-04 | norm: 0.6735 | dt: 2921.63ms | tok/sec: 2803.92\n",
            "step   290 | loss: 4.294711 | lr 4.9548e-04 | norm: 0.6718 | dt: 2941.34ms | tok/sec: 2785.13\n",
            "step   291 | loss: 4.313330 | lr 4.9481e-04 | norm: 0.7036 | dt: 2912.14ms | tok/sec: 2813.05\n",
            "step   292 | loss: 4.466540 | lr 4.9414e-04 | norm: 0.7667 | dt: 2911.56ms | tok/sec: 2813.62\n",
            "step   293 | loss: 4.309719 | lr 4.9347e-04 | norm: 0.7031 | dt: 2917.66ms | tok/sec: 2807.73\n",
            "step   294 | loss: 4.335454 | lr 4.9279e-04 | norm: 0.6548 | dt: 2922.00ms | tok/sec: 2803.56\n",
            "step   295 | loss: 4.251315 | lr 4.9211e-04 | norm: 0.6662 | dt: 2925.42ms | tok/sec: 2800.28\n",
            "step   296 | loss: 4.160529 | lr 4.9143e-04 | norm: 0.6329 | dt: 2904.54ms | tok/sec: 2820.42\n",
            "step   297 | loss: 4.317417 | lr 4.9075e-04 | norm: 0.6690 | dt: 2914.05ms | tok/sec: 2811.21\n",
            "step   298 | loss: 4.351947 | lr 4.9007e-04 | norm: 0.6866 | dt: 2905.51ms | tok/sec: 2819.47\n",
            "step   299 | loss: 4.460559 | lr 4.8939e-04 | norm: 0.7492 | dt: 2940.58ms | tok/sec: 2785.85\n",
            "step   300 | loss: 4.336652 | lr 4.8870e-04 | norm: 0.7535 | dt: 2908.33ms | tok/sec: 2816.73\n",
            "step   301 | loss: 4.257369 | lr 4.8802e-04 | norm: 0.6709 | dt: 2905.71ms | tok/sec: 2819.28\n",
            "step   302 | loss: 4.345538 | lr 4.8733e-04 | norm: 0.6893 | dt: 2898.93ms | tok/sec: 2825.87\n",
            "step   303 | loss: 4.280836 | lr 4.8664e-04 | norm: 0.7173 | dt: 2923.34ms | tok/sec: 2802.28\n",
            "step   304 | loss: 4.364259 | lr 4.8594e-04 | norm: 0.7229 | dt: 2902.43ms | tok/sec: 2822.47\n",
            "step   305 | loss: 4.203460 | lr 4.8525e-04 | norm: 0.7047 | dt: 2912.24ms | tok/sec: 2812.96\n",
            "step   306 | loss: 4.233839 | lr 4.8456e-04 | norm: 0.7005 | dt: 2916.01ms | tok/sec: 2809.31\n",
            "step   307 | loss: 4.383132 | lr 4.8386e-04 | norm: 0.8505 | dt: 2908.13ms | tok/sec: 2816.93\n",
            "step   308 | loss: 4.633745 | lr 4.8316e-04 | norm: 1.0146 | dt: 2912.78ms | tok/sec: 2812.44\n",
            "step   309 | loss: 4.258115 | lr 4.8246e-04 | norm: 0.7307 | dt: 2899.91ms | tok/sec: 2824.91\n",
            "step   310 | loss: 4.193232 | lr 4.8176e-04 | norm: 0.6905 | dt: 2895.72ms | tok/sec: 2829.01\n",
            "step   311 | loss: 4.391207 | lr 4.8106e-04 | norm: 0.7381 | dt: 2903.52ms | tok/sec: 2821.40\n",
            "step   312 | loss: 4.549798 | lr 4.8036e-04 | norm: 0.7130 | dt: 2917.86ms | tok/sec: 2807.54\n",
            "step   313 | loss: 4.351229 | lr 4.7965e-04 | norm: 0.6981 | dt: 2897.73ms | tok/sec: 2827.04\n",
            "step   314 | loss: 4.245054 | lr 4.7894e-04 | norm: 0.6720 | dt: 2902.45ms | tok/sec: 2822.45\n",
            "step   315 | loss: 4.429837 | lr 4.7824e-04 | norm: 0.7555 | dt: 2908.69ms | tok/sec: 2816.39\n",
            "step   316 | loss: 4.146920 | lr 4.7753e-04 | norm: 0.7116 | dt: 2929.82ms | tok/sec: 2796.08\n",
            "step   317 | loss: 4.177200 | lr 4.7682e-04 | norm: 0.7329 | dt: 2900.43ms | tok/sec: 2824.41\n",
            "step   318 | loss: 4.131460 | lr 4.7610e-04 | norm: 0.7174 | dt: 2911.55ms | tok/sec: 2813.62\n",
            "step   319 | loss: 3.991684 | lr 4.7539e-04 | norm: 0.6857 | dt: 2917.54ms | tok/sec: 2807.84\n",
            "step   320 | loss: 4.326186 | lr 4.7467e-04 | norm: 0.7634 | dt: 2917.97ms | tok/sec: 2807.43\n",
            "step   321 | loss: 4.221874 | lr 4.7396e-04 | norm: 0.7151 | dt: 2913.24ms | tok/sec: 2811.99\n",
            "step   322 | loss: 4.160071 | lr 4.7324e-04 | norm: 0.6903 | dt: 2904.02ms | tok/sec: 2820.92\n",
            "step   323 | loss: 4.166413 | lr 4.7252e-04 | norm: 0.7028 | dt: 2912.10ms | tok/sec: 2813.09\n",
            "step   324 | loss: 4.147594 | lr 4.7180e-04 | norm: 0.6892 | dt: 2907.36ms | tok/sec: 2817.68\n",
            "step   325 | loss: 4.208488 | lr 4.7107e-04 | norm: 0.6823 | dt: 2917.72ms | tok/sec: 2807.67\n",
            "step   326 | loss: 4.156865 | lr 4.7035e-04 | norm: 0.6773 | dt: 2916.81ms | tok/sec: 2808.55\n",
            "step   327 | loss: 4.283228 | lr 4.6963e-04 | norm: 0.7820 | dt: 2910.00ms | tok/sec: 2815.12\n",
            "step   328 | loss: 4.156019 | lr 4.6890e-04 | norm: 0.6764 | dt: 2913.96ms | tok/sec: 2811.29\n",
            "step   329 | loss: 4.105617 | lr 4.6817e-04 | norm: 0.6794 | dt: 2913.51ms | tok/sec: 2811.73\n",
            "step   330 | loss: 4.348289 | lr 4.6744e-04 | norm: 0.7256 | dt: 2918.55ms | tok/sec: 2806.87\n",
            "step   331 | loss: 4.085595 | lr 4.6671e-04 | norm: 0.7067 | dt: 2904.54ms | tok/sec: 2820.41\n",
            "step   332 | loss: 4.372523 | lr 4.6598e-04 | norm: 0.7717 | dt: 2901.94ms | tok/sec: 2822.93\n",
            "step   333 | loss: 4.030875 | lr 4.6524e-04 | norm: 0.6747 | dt: 2936.22ms | tok/sec: 2789.98\n",
            "step   334 | loss: 4.309076 | lr 4.6451e-04 | norm: 0.7464 | dt: 2906.03ms | tok/sec: 2818.96\n",
            "step   335 | loss: 4.132498 | lr 4.6377e-04 | norm: 0.6793 | dt: 2913.97ms | tok/sec: 2811.29\n",
            "step   336 | loss: 4.228363 | lr 4.6304e-04 | norm: 0.7495 | dt: 2908.72ms | tok/sec: 2816.36\n",
            "step   337 | loss: 4.289398 | lr 4.6230e-04 | norm: 0.7136 | dt: 2924.16ms | tok/sec: 2801.49\n",
            "step   338 | loss: 4.156998 | lr 4.6156e-04 | norm: 0.7646 | dt: 2917.73ms | tok/sec: 2807.66\n",
            "step   339 | loss: 4.091673 | lr 4.6082e-04 | norm: 0.6790 | dt: 2909.32ms | tok/sec: 2815.78\n",
            "step   340 | loss: 4.215327 | lr 4.6007e-04 | norm: 0.7093 | dt: 2910.40ms | tok/sec: 2814.73\n",
            "step   341 | loss: 4.146128 | lr 4.5933e-04 | norm: 0.7947 | dt: 2906.72ms | tok/sec: 2818.30\n",
            "step   342 | loss: 4.210685 | lr 4.5858e-04 | norm: 0.7588 | dt: 2909.60ms | tok/sec: 2815.51\n",
            "step   343 | loss: 4.144630 | lr 4.5784e-04 | norm: 0.7259 | dt: 2908.97ms | tok/sec: 2816.12\n",
            "step   344 | loss: 4.148129 | lr 4.5709e-04 | norm: 0.7251 | dt: 2908.58ms | tok/sec: 2816.49\n",
            "step   345 | loss: 4.172826 | lr 4.5634e-04 | norm: 0.7145 | dt: 2912.11ms | tok/sec: 2813.08\n",
            "step   346 | loss: 4.031871 | lr 4.5559e-04 | norm: 0.6527 | dt: 2911.30ms | tok/sec: 2813.86\n",
            "step   347 | loss: 4.245964 | lr 4.5484e-04 | norm: 0.7048 | dt: 2910.92ms | tok/sec: 2814.23\n",
            "step   348 | loss: 4.093524 | lr 4.5409e-04 | norm: 0.6872 | dt: 2911.49ms | tok/sec: 2813.68\n",
            "step   349 | loss: 4.172867 | lr 4.5333e-04 | norm: 0.7119 | dt: 2917.54ms | tok/sec: 2807.84\n",
            "step   350 | loss: 4.206991 | lr 4.5258e-04 | norm: 0.7176 | dt: 2922.23ms | tok/sec: 2803.34\n",
            "step   351 | loss: 4.173372 | lr 4.5182e-04 | norm: 0.7339 | dt: 2895.75ms | tok/sec: 2828.97\n",
            "step   352 | loss: 4.192896 | lr 4.5106e-04 | norm: 0.7133 | dt: 2908.29ms | tok/sec: 2816.78\n",
            "step   353 | loss: 4.253617 | lr 4.5030e-04 | norm: 0.7694 | dt: 2901.04ms | tok/sec: 2823.82\n",
            "step   354 | loss: 4.075354 | lr 4.4954e-04 | norm: 0.7157 | dt: 2924.61ms | tok/sec: 2801.06\n",
            "step   355 | loss: 4.195923 | lr 4.4878e-04 | norm: 0.7322 | dt: 2904.13ms | tok/sec: 2820.81\n",
            "step   356 | loss: 4.117265 | lr 4.4802e-04 | norm: 0.7054 | dt: 2917.08ms | tok/sec: 2808.28\n",
            "step   357 | loss: 4.074380 | lr 4.4726e-04 | norm: 0.7076 | dt: 2906.70ms | tok/sec: 2818.31\n",
            "step   358 | loss: 4.170949 | lr 4.4649e-04 | norm: 0.7200 | dt: 2906.88ms | tok/sec: 2818.14\n",
            "step   359 | loss: 4.079406 | lr 4.4573e-04 | norm: 0.6994 | dt: 2906.92ms | tok/sec: 2818.10\n",
            "step   360 | loss: 4.118739 | lr 4.4496e-04 | norm: 0.7341 | dt: 2910.16ms | tok/sec: 2814.97\n",
            "step   361 | loss: 4.028753 | lr 4.4419e-04 | norm: 0.7098 | dt: 2916.61ms | tok/sec: 2808.74\n",
            "step   362 | loss: 3.972956 | lr 4.4342e-04 | norm: 0.6907 | dt: 2905.22ms | tok/sec: 2819.75\n",
            "step   363 | loss: 3.950940 | lr 4.4265e-04 | norm: 0.6953 | dt: 2930.45ms | tok/sec: 2795.47\n",
            "step   364 | loss: 4.060718 | lr 4.4188e-04 | norm: 0.7468 | dt: 2896.67ms | tok/sec: 2828.07\n",
            "step   365 | loss: 4.065580 | lr 4.4111e-04 | norm: 0.7363 | dt: 2915.18ms | tok/sec: 2810.12\n",
            "step   366 | loss: 4.147702 | lr 4.4034e-04 | norm: 0.7149 | dt: 2917.17ms | tok/sec: 2808.20\n",
            "step   367 | loss: 4.039879 | lr 4.3956e-04 | norm: 0.6844 | dt: 2933.67ms | tok/sec: 2792.41\n",
            "step   368 | loss: 4.166116 | lr 4.3878e-04 | norm: 0.7112 | dt: 2912.84ms | tok/sec: 2812.38\n",
            "step   369 | loss: 3.993520 | lr 4.3801e-04 | norm: 0.6801 | dt: 2907.30ms | tok/sec: 2817.74\n",
            "step   370 | loss: 4.063554 | lr 4.3723e-04 | norm: 0.6984 | dt: 2907.15ms | tok/sec: 2817.88\n",
            "step   371 | loss: 4.141210 | lr 4.3645e-04 | norm: 0.6841 | dt: 2910.78ms | tok/sec: 2814.37\n",
            "step   372 | loss: 4.030849 | lr 4.3567e-04 | norm: 0.6885 | dt: 2906.94ms | tok/sec: 2818.08\n",
            "step   373 | loss: 4.266032 | lr 4.3489e-04 | norm: 0.7512 | dt: 2914.78ms | tok/sec: 2810.50\n",
            "step   374 | loss: 4.130704 | lr 4.3411e-04 | norm: 0.7349 | dt: 2913.04ms | tok/sec: 2812.18\n",
            "step   375 | loss: 4.025272 | lr 4.3332e-04 | norm: 0.7180 | dt: 2920.59ms | tok/sec: 2804.91\n",
            "step   376 | loss: 3.986638 | lr 4.3254e-04 | norm: 0.7546 | dt: 2908.74ms | tok/sec: 2816.34\n",
            "step   377 | loss: 4.113675 | lr 4.3176e-04 | norm: 0.8219 | dt: 2922.94ms | tok/sec: 2802.66\n",
            "step   378 | loss: 4.184750 | lr 4.3097e-04 | norm: 0.8736 | dt: 2913.43ms | tok/sec: 2811.81\n",
            "step   379 | loss: 3.961632 | lr 4.3018e-04 | norm: 0.7027 | dt: 2908.11ms | tok/sec: 2816.95\n",
            "step   380 | loss: 4.063189 | lr 4.2939e-04 | norm: 0.7226 | dt: 2938.68ms | tok/sec: 2787.64\n",
            "step   381 | loss: 4.153375 | lr 4.2860e-04 | norm: 0.7299 | dt: 2912.51ms | tok/sec: 2812.69\n",
            "step   382 | loss: 4.055120 | lr 4.2781e-04 | norm: 0.7071 | dt: 2909.72ms | tok/sec: 2815.39\n",
            "step   383 | loss: 3.982370 | lr 4.2702e-04 | norm: 0.6752 | dt: 2903.83ms | tok/sec: 2821.10\n",
            "step   384 | loss: 4.042427 | lr 4.2623e-04 | norm: 0.7262 | dt: 2915.48ms | tok/sec: 2809.83\n",
            "step   385 | loss: 4.014966 | lr 4.2544e-04 | norm: 0.7420 | dt: 2914.52ms | tok/sec: 2810.75\n",
            "step   386 | loss: 3.857497 | lr 4.2464e-04 | norm: 0.7185 | dt: 2919.75ms | tok/sec: 2805.71\n",
            "step   387 | loss: 4.065618 | lr 4.2385e-04 | norm: 0.8444 | dt: 2912.20ms | tok/sec: 2813.00\n",
            "step   388 | loss: 3.895448 | lr 4.2305e-04 | norm: 0.7142 | dt: 2931.56ms | tok/sec: 2794.41\n",
            "step   389 | loss: 4.012123 | lr 4.2226e-04 | norm: 0.7382 | dt: 2916.25ms | tok/sec: 2809.08\n",
            "step   390 | loss: 3.979675 | lr 4.2146e-04 | norm: 0.7552 | dt: 2910.79ms | tok/sec: 2814.36\n",
            "step   391 | loss: 4.008001 | lr 4.2066e-04 | norm: 0.7217 | dt: 2911.28ms | tok/sec: 2813.89\n",
            "step   392 | loss: 4.005798 | lr 4.1986e-04 | norm: 0.7532 | dt: 2915.90ms | tok/sec: 2809.43\n",
            "step   393 | loss: 4.175773 | lr 4.1906e-04 | norm: 0.7909 | dt: 2944.82ms | tok/sec: 2781.83\n",
            "step   394 | loss: 3.970309 | lr 4.1826e-04 | norm: 0.7200 | dt: 2916.98ms | tok/sec: 2808.38\n",
            "step   395 | loss: 3.902658 | lr 4.1746e-04 | norm: 0.6773 | dt: 2903.83ms | tok/sec: 2821.10\n",
            "step   396 | loss: 3.939240 | lr 4.1665e-04 | norm: 0.7279 | dt: 2912.05ms | tok/sec: 2813.13\n",
            "step   397 | loss: 3.959561 | lr 4.1585e-04 | norm: 0.6927 | dt: 2927.38ms | tok/sec: 2798.41\n",
            "step   398 | loss: 3.905492 | lr 4.1505e-04 | norm: 0.7182 | dt: 2902.28ms | tok/sec: 2822.61\n",
            "step   399 | loss: 3.935061 | lr 4.1424e-04 | norm: 0.7645 | dt: 2919.57ms | tok/sec: 2805.89\n",
            "step   400 | loss: 3.855771 | lr 4.1343e-04 | norm: 0.6782 | dt: 2910.55ms | tok/sec: 2814.59\n",
            "step   401 | loss: 4.029262 | lr 4.1263e-04 | norm: 0.7811 | dt: 2925.69ms | tok/sec: 2800.02\n",
            "step   402 | loss: 4.035452 | lr 4.1182e-04 | norm: 0.7508 | dt: 2910.24ms | tok/sec: 2814.89\n",
            "step   403 | loss: 3.760787 | lr 4.1101e-04 | norm: 0.6781 | dt: 2906.00ms | tok/sec: 2819.00\n",
            "step   404 | loss: 3.911529 | lr 4.1020e-04 | norm: 0.7124 | dt: 2911.92ms | tok/sec: 2813.27\n",
            "step   405 | loss: 3.820645 | lr 4.0939e-04 | norm: 0.6859 | dt: 2932.97ms | tok/sec: 2793.07\n",
            "step   406 | loss: 3.943236 | lr 4.0858e-04 | norm: 0.7040 | dt: 2907.65ms | tok/sec: 2817.40\n",
            "step   407 | loss: 3.857112 | lr 4.0777e-04 | norm: 0.7030 | dt: 2914.13ms | tok/sec: 2811.13\n",
            "step   408 | loss: 3.905668 | lr 4.0696e-04 | norm: 0.7095 | dt: 2910.77ms | tok/sec: 2814.37\n",
            "step   409 | loss: 4.092970 | lr 4.0614e-04 | norm: 0.7634 | dt: 2925.26ms | tok/sec: 2800.43\n",
            "step   410 | loss: 4.015785 | lr 4.0533e-04 | norm: 0.7418 | dt: 2914.01ms | tok/sec: 2811.24\n",
            "step   411 | loss: 3.728017 | lr 4.0451e-04 | norm: 0.6528 | dt: 2905.41ms | tok/sec: 2819.56\n",
            "step   412 | loss: 3.993516 | lr 4.0370e-04 | norm: 0.7385 | dt: 2904.97ms | tok/sec: 2820.00\n",
            "step   413 | loss: 3.986303 | lr 4.0288e-04 | norm: 0.7762 | dt: 2913.31ms | tok/sec: 2811.92\n",
            "step   414 | loss: 3.693567 | lr 4.0206e-04 | norm: 0.6644 | dt: 2916.28ms | tok/sec: 2809.06\n",
            "step   415 | loss: 3.980512 | lr 4.0125e-04 | norm: 0.7210 | dt: 2923.87ms | tok/sec: 2801.77\n",
            "step   416 | loss: 3.852141 | lr 4.0043e-04 | norm: 0.7580 | dt: 2916.14ms | tok/sec: 2809.20\n",
            "step   417 | loss: 3.982772 | lr 3.9961e-04 | norm: 0.8320 | dt: 2905.07ms | tok/sec: 2819.89\n",
            "step   418 | loss: 3.952287 | lr 3.9879e-04 | norm: 0.7531 | dt: 2942.12ms | tok/sec: 2784.39\n",
            "step   419 | loss: 3.957546 | lr 3.9797e-04 | norm: 0.7259 | dt: 2925.63ms | tok/sec: 2800.08\n",
            "step   420 | loss: 3.796005 | lr 3.9715e-04 | norm: 0.7280 | dt: 2915.48ms | tok/sec: 2809.83\n",
            "step   421 | loss: 3.785491 | lr 3.9632e-04 | norm: 0.7039 | dt: 2908.18ms | tok/sec: 2816.89\n",
            "step   422 | loss: 3.892327 | lr 3.9550e-04 | norm: 0.7837 | dt: 2929.70ms | tok/sec: 2796.19\n",
            "step   423 | loss: 3.871825 | lr 3.9468e-04 | norm: 0.7521 | dt: 2909.45ms | tok/sec: 2815.66\n",
            "step   424 | loss: 3.842515 | lr 3.9385e-04 | norm: 0.7338 | dt: 2896.62ms | tok/sec: 2828.12\n",
            "step   425 | loss: 3.706482 | lr 3.9303e-04 | norm: 0.7260 | dt: 2907.38ms | tok/sec: 2817.66\n",
            "step   426 | loss: 3.922158 | lr 3.9221e-04 | norm: 0.7757 | dt: 2923.85ms | tok/sec: 2801.79\n",
            "step   427 | loss: 3.910141 | lr 3.9138e-04 | norm: 0.7629 | dt: 2936.82ms | tok/sec: 2789.41\n",
            "step   428 | loss: 3.850137 | lr 3.9055e-04 | norm: 0.7441 | dt: 2913.06ms | tok/sec: 2812.16\n",
            "step   429 | loss: 3.966175 | lr 3.8973e-04 | norm: 0.7473 | dt: 2915.61ms | tok/sec: 2809.70\n",
            "step   430 | loss: 3.930548 | lr 3.8890e-04 | norm: 0.7617 | dt: 2903.73ms | tok/sec: 2821.20\n",
            "step   431 | loss: 3.935681 | lr 3.8807e-04 | norm: 0.8613 | dt: 2908.63ms | tok/sec: 2816.45\n",
            "step   432 | loss: 3.913484 | lr 3.8724e-04 | norm: 0.7776 | dt: 2907.59ms | tok/sec: 2817.46\n",
            "step   433 | loss: 3.909835 | lr 3.8641e-04 | norm: 0.8092 | dt: 2909.03ms | tok/sec: 2816.05\n",
            "step   434 | loss: 4.013188 | lr 3.8558e-04 | norm: 0.8486 | dt: 2915.11ms | tok/sec: 2810.19\n",
            "step   435 | loss: 3.858881 | lr 3.8475e-04 | norm: 0.7837 | dt: 2922.72ms | tok/sec: 2802.87\n",
            "step   436 | loss: 3.868958 | lr 3.8392e-04 | norm: 0.7476 | dt: 2928.11ms | tok/sec: 2797.71\n",
            "step   437 | loss: 3.843314 | lr 3.8309e-04 | norm: 0.7380 | dt: 2906.56ms | tok/sec: 2818.45\n",
            "step   438 | loss: 3.778608 | lr 3.8226e-04 | norm: 0.7543 | dt: 2915.73ms | tok/sec: 2809.59\n",
            "step   439 | loss: 3.998493 | lr 3.8143e-04 | norm: 0.8160 | dt: 2912.93ms | tok/sec: 2812.29\n",
            "step   440 | loss: 3.881061 | lr 3.8059e-04 | norm: 0.7612 | dt: 2907.94ms | tok/sec: 2817.11\n",
            "step   441 | loss: 3.869041 | lr 3.7976e-04 | norm: 0.7296 | dt: 2910.38ms | tok/sec: 2814.75\n",
            "step   442 | loss: 3.763919 | lr 3.7893e-04 | norm: 0.7444 | dt: 2909.54ms | tok/sec: 2815.57\n",
            "step   443 | loss: 3.829922 | lr 3.7809e-04 | norm: 0.7533 | dt: 2908.05ms | tok/sec: 2817.01\n",
            "step   444 | loss: 3.854918 | lr 3.7726e-04 | norm: 0.7707 | dt: 2938.81ms | tok/sec: 2787.52\n",
            "step   445 | loss: 3.723236 | lr 3.7642e-04 | norm: 0.7113 | dt: 2905.35ms | tok/sec: 2819.63\n",
            "step   446 | loss: 3.744414 | lr 3.7559e-04 | norm: 0.7569 | dt: 2905.60ms | tok/sec: 2819.38\n",
            "step   447 | loss: 3.858833 | lr 3.7475e-04 | norm: 0.8208 | dt: 2909.69ms | tok/sec: 2815.42\n",
            "step   448 | loss: 3.683028 | lr 3.7391e-04 | norm: 0.7437 | dt: 2923.66ms | tok/sec: 2801.97\n",
            "step   449 | loss: 3.621260 | lr 3.7307e-04 | norm: 0.7137 | dt: 2902.75ms | tok/sec: 2822.16\n",
            "step   450 | loss: 3.904104 | lr 3.7224e-04 | norm: 0.8975 | dt: 2896.31ms | tok/sec: 2828.43\n",
            "step   451 | loss: 3.864200 | lr 3.7140e-04 | norm: 0.7830 | dt: 2903.75ms | tok/sec: 2821.18\n",
            "step   452 | loss: 3.925107 | lr 3.7056e-04 | norm: 0.8136 | dt: 2905.01ms | tok/sec: 2819.96\n",
            "step   453 | loss: 3.685108 | lr 3.6972e-04 | norm: 0.7838 | dt: 2898.09ms | tok/sec: 2826.68\n",
            "step   454 | loss: 3.708802 | lr 3.6888e-04 | norm: 0.7382 | dt: 2903.61ms | tok/sec: 2821.31\n",
            "step   455 | loss: 3.679904 | lr 3.6804e-04 | norm: 0.7331 | dt: 2913.65ms | tok/sec: 2811.60\n",
            "step   456 | loss: 3.766681 | lr 3.6720e-04 | norm: 0.7713 | dt: 2914.81ms | tok/sec: 2810.47\n",
            "step   457 | loss: 3.611247 | lr 3.6636e-04 | norm: 0.7545 | dt: 2903.35ms | tok/sec: 2821.57\n",
            "step   458 | loss: 3.784817 | lr 3.6552e-04 | norm: 0.7772 | dt: 2922.11ms | tok/sec: 2803.46\n",
            "step   459 | loss: 3.724982 | lr 3.6468e-04 | norm: 0.8067 | dt: 2910.25ms | tok/sec: 2814.88\n",
            "step   460 | loss: 3.882337 | lr 3.6384e-04 | norm: 0.8224 | dt: 2904.81ms | tok/sec: 2820.15\n",
            "step   461 | loss: 3.804026 | lr 3.6300e-04 | norm: 0.7853 | dt: 2924.28ms | tok/sec: 2801.37\n",
            "step   462 | loss: 3.878129 | lr 3.6216e-04 | norm: 0.7639 | dt: 2909.20ms | tok/sec: 2815.89\n",
            "step   463 | loss: 3.712270 | lr 3.6131e-04 | norm: 0.7189 | dt: 2907.85ms | tok/sec: 2817.20\n",
            "step   464 | loss: 3.643849 | lr 3.6047e-04 | norm: 0.7065 | dt: 2902.67ms | tok/sec: 2822.23\n",
            "step   465 | loss: 3.994843 | lr 3.5963e-04 | norm: 1.2922 | dt: 2919.81ms | tok/sec: 2805.66\n",
            "step   466 | loss: 3.877804 | lr 3.5879e-04 | norm: 0.7776 | dt: 2906.81ms | tok/sec: 2818.21\n",
            "step   467 | loss: 3.578898 | lr 3.5794e-04 | norm: 0.7062 | dt: 2897.61ms | tok/sec: 2827.16\n",
            "step   468 | loss: 3.689844 | lr 3.5710e-04 | norm: 0.7270 | dt: 2909.07ms | tok/sec: 2816.02\n",
            "step   469 | loss: 3.518663 | lr 3.5625e-04 | norm: 0.7142 | dt: 2928.46ms | tok/sec: 2797.37\n",
            "step   470 | loss: 3.790060 | lr 3.5541e-04 | norm: 0.7959 | dt: 2914.18ms | tok/sec: 2811.08\n",
            "step   471 | loss: 3.705521 | lr 3.5456e-04 | norm: 0.7742 | dt: 2904.01ms | tok/sec: 2820.93\n",
            "step   472 | loss: 3.748662 | lr 3.5372e-04 | norm: 0.8082 | dt: 2905.59ms | tok/sec: 2819.40\n",
            "step   473 | loss: 3.810462 | lr 3.5287e-04 | norm: 0.8471 | dt: 2914.34ms | tok/sec: 2810.93\n",
            "step   474 | loss: 3.759959 | lr 3.5203e-04 | norm: 0.7946 | dt: 2905.65ms | tok/sec: 2819.33\n",
            "step   475 | loss: 3.700799 | lr 3.5118e-04 | norm: 0.7672 | dt: 2915.76ms | tok/sec: 2809.56\n",
            "step   476 | loss: 3.624181 | lr 3.5034e-04 | norm: 0.7425 | dt: 2915.69ms | tok/sec: 2809.63\n",
            "step   477 | loss: 3.628613 | lr 3.4949e-04 | norm: 0.7399 | dt: 2907.64ms | tok/sec: 2817.40\n",
            "step   478 | loss: 3.644476 | lr 3.4865e-04 | norm: 0.7262 | dt: 2917.48ms | tok/sec: 2807.90\n",
            "step   479 | loss: 3.764530 | lr 3.4780e-04 | norm: 0.7377 | dt: 2909.85ms | tok/sec: 2815.27\n",
            "step   480 | loss: 3.625165 | lr 3.4695e-04 | norm: 0.7682 | dt: 2908.33ms | tok/sec: 2816.74\n",
            "step   481 | loss: 3.615777 | lr 3.4611e-04 | norm: 0.7337 | dt: 2906.06ms | tok/sec: 2818.94\n",
            "step   482 | loss: 3.684288 | lr 3.4526e-04 | norm: 0.7377 | dt: 2912.50ms | tok/sec: 2812.70\n",
            "step   483 | loss: 3.721396 | lr 3.4441e-04 | norm: 0.9416 | dt: 2911.04ms | tok/sec: 2814.12\n",
            "step   484 | loss: 3.580699 | lr 3.4357e-04 | norm: 0.7181 | dt: 2909.35ms | tok/sec: 2815.75\n",
            "step   485 | loss: 3.843622 | lr 3.4272e-04 | norm: 0.8245 | dt: 2905.04ms | tok/sec: 2819.93\n",
            "step   486 | loss: 3.731943 | lr 3.4187e-04 | norm: 0.7558 | dt: 2918.89ms | tok/sec: 2806.55\n",
            "step   487 | loss: 3.637759 | lr 3.4102e-04 | norm: 0.7622 | dt: 2904.40ms | tok/sec: 2820.54\n",
            "step   488 | loss: 3.528368 | lr 3.4018e-04 | norm: 0.7158 | dt: 2901.25ms | tok/sec: 2823.61\n",
            "step   489 | loss: 3.650379 | lr 3.3933e-04 | norm: 0.7733 | dt: 2905.46ms | tok/sec: 2819.52\n",
            "step   490 | loss: 3.678078 | lr 3.3848e-04 | norm: 0.7577 | dt: 2913.21ms | tok/sec: 2812.02\n",
            "step   491 | loss: 3.630117 | lr 3.3763e-04 | norm: 0.7566 | dt: 2909.01ms | tok/sec: 2816.08\n",
            "step   492 | loss: 3.665024 | lr 3.3679e-04 | norm: 0.7389 | dt: 2909.93ms | tok/sec: 2815.19\n",
            "step   493 | loss: 3.514925 | lr 3.3594e-04 | norm: 0.7238 | dt: 2898.56ms | tok/sec: 2826.24\n",
            "step   494 | loss: 3.632638 | lr 3.3509e-04 | norm: 0.7300 | dt: 2907.26ms | tok/sec: 2817.78\n",
            "step   495 | loss: 3.684684 | lr 3.3424e-04 | norm: 0.7652 | dt: 2911.12ms | tok/sec: 2814.04\n",
            "step   496 | loss: 3.443600 | lr 3.3339e-04 | norm: 0.7086 | dt: 2896.53ms | tok/sec: 2828.21\n",
            "step   497 | loss: 3.665495 | lr 3.3254e-04 | norm: 0.7856 | dt: 2907.44ms | tok/sec: 2817.60\n",
            "step   498 | loss: 3.577341 | lr 3.3170e-04 | norm: 0.7993 | dt: 2916.65ms | tok/sec: 2808.70\n",
            "step   499 | loss: 3.665729 | lr 3.3085e-04 | norm: 0.8074 | dt: 2913.68ms | tok/sec: 2811.56\n",
            "step   500 | loss: 3.600495 | lr 3.3000e-04 | norm: 0.7873 | dt: 8752.27ms | tok/sec: 935.99\n",
            "step   501 | loss: 3.663564 | lr 3.2915e-04 | norm: 0.7832 | dt: 2890.17ms | tok/sec: 2834.43\n",
            "step   502 | loss: 3.485777 | lr 3.2830e-04 | norm: 0.6945 | dt: 2885.13ms | tok/sec: 2839.38\n",
            "step   503 | loss: 3.604462 | lr 3.2746e-04 | norm: 0.7335 | dt: 2888.87ms | tok/sec: 2835.71\n",
            "step   504 | loss: 3.570822 | lr 3.2661e-04 | norm: 0.6985 | dt: 2913.41ms | tok/sec: 2811.83\n",
            "step   505 | loss: 3.851179 | lr 3.2576e-04 | norm: 0.7972 | dt: 2914.96ms | tok/sec: 2810.33\n",
            "step   506 | loss: 3.775319 | lr 3.2491e-04 | norm: 0.7721 | dt: 2930.74ms | tok/sec: 2795.20\n",
            "step   507 | loss: 3.507839 | lr 3.2406e-04 | norm: 0.7196 | dt: 2931.57ms | tok/sec: 2794.41\n",
            "step   508 | loss: 3.791171 | lr 3.2321e-04 | norm: 0.7759 | dt: 2943.87ms | tok/sec: 2782.74\n",
            "step   509 | loss: 3.465139 | lr 3.2237e-04 | norm: 0.7178 | dt: 2948.47ms | tok/sec: 2778.39\n",
            "step   510 | loss: 3.538754 | lr 3.2152e-04 | norm: 0.7877 | dt: 2951.63ms | tok/sec: 2775.42\n",
            "step   511 | loss: 3.578304 | lr 3.2067e-04 | norm: 0.7464 | dt: 2946.03ms | tok/sec: 2780.69\n",
            "step   512 | loss: 3.335834 | lr 3.1982e-04 | norm: 0.6510 | dt: 2929.99ms | tok/sec: 2795.92\n",
            "step   513 | loss: 3.750294 | lr 3.1898e-04 | norm: 0.9188 | dt: 2940.32ms | tok/sec: 2786.09\n",
            "step   514 | loss: 3.749525 | lr 3.1813e-04 | norm: 0.8523 | dt: 2939.99ms | tok/sec: 2786.40\n",
            "step   515 | loss: 3.683086 | lr 3.1728e-04 | norm: 0.8669 | dt: 2921.60ms | tok/sec: 2803.94\n",
            "step   516 | loss: 3.496170 | lr 3.1643e-04 | norm: 0.7371 | dt: 2926.11ms | tok/sec: 2799.62\n",
            "step   517 | loss: 3.494838 | lr 3.1559e-04 | norm: 0.7328 | dt: 2917.52ms | tok/sec: 2807.86\n",
            "step   518 | loss: 3.585266 | lr 3.1474e-04 | norm: 0.7088 | dt: 2926.76ms | tok/sec: 2799.00\n",
            "step   519 | loss: 3.466930 | lr 3.1389e-04 | norm: 0.7598 | dt: 2913.16ms | tok/sec: 2812.07\n",
            "step   520 | loss: 3.495546 | lr 3.1305e-04 | norm: 0.7432 | dt: 2903.97ms | tok/sec: 2820.97\n",
            "step   521 | loss: 3.589995 | lr 3.1220e-04 | norm: 0.7489 | dt: 2898.28ms | tok/sec: 2826.51\n",
            "step   522 | loss: 3.798937 | lr 3.1135e-04 | norm: 0.8617 | dt: 2902.75ms | tok/sec: 2822.16\n",
            "step   523 | loss: 3.642666 | lr 3.1051e-04 | norm: 0.7742 | dt: 2917.51ms | tok/sec: 2807.88\n",
            "step   524 | loss: 3.688269 | lr 3.0966e-04 | norm: 0.8303 | dt: 2894.64ms | tok/sec: 2830.06\n",
            "step   525 | loss: 3.663918 | lr 3.0882e-04 | norm: 0.8337 | dt: 2888.11ms | tok/sec: 2836.46\n",
            "step   526 | loss: 3.491917 | lr 3.0797e-04 | norm: 0.7329 | dt: 2886.54ms | tok/sec: 2838.00\n",
            "step   527 | loss: 3.519869 | lr 3.0713e-04 | norm: 0.9899 | dt: 2903.21ms | tok/sec: 2821.71\n",
            "step   528 | loss: 3.523963 | lr 3.0628e-04 | norm: 0.7655 | dt: 2901.34ms | tok/sec: 2823.52\n",
            "step   529 | loss: 3.729551 | lr 3.0544e-04 | norm: 0.8229 | dt: 2900.66ms | tok/sec: 2824.18\n",
            "step   530 | loss: 3.455373 | lr 3.0459e-04 | norm: 0.7194 | dt: 2899.84ms | tok/sec: 2824.98\n",
            "step   531 | loss: 3.497639 | lr 3.0375e-04 | norm: 0.7515 | dt: 2915.95ms | tok/sec: 2809.37\n",
            "step   532 | loss: 3.689981 | lr 3.0290e-04 | norm: 0.8330 | dt: 2912.49ms | tok/sec: 2812.71\n",
            "step   533 | loss: 3.789336 | lr 3.0206e-04 | norm: 0.8022 | dt: 2908.74ms | tok/sec: 2816.34\n",
            "step   534 | loss: 3.586430 | lr 3.0121e-04 | norm: 0.7475 | dt: 2907.77ms | tok/sec: 2817.28\n",
            "step   535 | loss: 3.493666 | lr 3.0037e-04 | norm: 0.7265 | dt: 2916.54ms | tok/sec: 2808.81\n",
            "step   536 | loss: 3.550636 | lr 2.9953e-04 | norm: 0.7839 | dt: 2928.93ms | tok/sec: 2796.92\n",
            "step   537 | loss: 3.766122 | lr 2.9869e-04 | norm: 0.8166 | dt: 2925.22ms | tok/sec: 2800.47\n",
            "step   538 | loss: 3.631034 | lr 2.9784e-04 | norm: 0.7684 | dt: 2926.80ms | tok/sec: 2798.96\n",
            "step   539 | loss: 3.484542 | lr 2.9700e-04 | norm: 0.7451 | dt: 2917.80ms | tok/sec: 2807.59\n",
            "step   540 | loss: 3.371042 | lr 2.9616e-04 | norm: 0.7351 | dt: 2919.50ms | tok/sec: 2805.96\n",
            "step   541 | loss: 3.571149 | lr 2.9532e-04 | norm: 0.7852 | dt: 2917.44ms | tok/sec: 2807.94\n",
            "step   542 | loss: 3.375491 | lr 2.9448e-04 | norm: 0.7331 | dt: 2913.69ms | tok/sec: 2811.55\n",
            "step   543 | loss: 3.423652 | lr 2.9364e-04 | norm: 0.7793 | dt: 2907.58ms | tok/sec: 2817.46\n",
            "step   544 | loss: 3.451119 | lr 2.9280e-04 | norm: 0.7425 | dt: 2933.34ms | tok/sec: 2792.72\n",
            "step   545 | loss: 3.581842 | lr 2.9196e-04 | norm: 0.8114 | dt: 2926.34ms | tok/sec: 2799.40\n",
            "step   546 | loss: 3.381243 | lr 2.9112e-04 | norm: 0.7142 | dt: 2913.06ms | tok/sec: 2812.16\n",
            "step   547 | loss: 3.470602 | lr 2.9028e-04 | norm: 0.7466 | dt: 2910.24ms | tok/sec: 2814.89\n",
            "step   548 | loss: 3.364388 | lr 2.8944e-04 | norm: 0.7176 | dt: 2941.84ms | tok/sec: 2784.66\n",
            "step   549 | loss: 3.446455 | lr 2.8860e-04 | norm: 0.7042 | dt: 2896.63ms | tok/sec: 2828.11\n",
            "step   550 | loss: 3.445337 | lr 2.8776e-04 | norm: 0.7180 | dt: 2910.36ms | tok/sec: 2814.78\n",
            "step   551 | loss: 3.439506 | lr 2.8693e-04 | norm: 0.7577 | dt: 2910.38ms | tok/sec: 2814.76\n",
            "step   552 | loss: 3.788106 | lr 2.8609e-04 | norm: 0.9213 | dt: 2906.27ms | tok/sec: 2818.73\n",
            "step   553 | loss: 3.411922 | lr 2.8525e-04 | norm: 0.7707 | dt: 2892.35ms | tok/sec: 2832.29\n",
            "step   554 | loss: 3.634273 | lr 2.8441e-04 | norm: 0.8448 | dt: 2916.04ms | tok/sec: 2809.29\n",
            "step   555 | loss: 3.703953 | lr 2.8358e-04 | norm: 0.8371 | dt: 2903.38ms | tok/sec: 2821.54\n",
            "step   556 | loss: 3.399259 | lr 2.8274e-04 | norm: 0.7333 | dt: 2897.75ms | tok/sec: 2827.02\n",
            "step   557 | loss: 3.396641 | lr 2.8191e-04 | norm: 0.7156 | dt: 2940.91ms | tok/sec: 2785.53\n",
            "step   558 | loss: 3.372731 | lr 2.8107e-04 | norm: 0.7077 | dt: 2911.72ms | tok/sec: 2813.45\n",
            "step   559 | loss: 3.678996 | lr 2.8024e-04 | norm: 0.8359 | dt: 2890.52ms | tok/sec: 2834.09\n",
            "step   560 | loss: 3.393498 | lr 2.7941e-04 | norm: 0.7148 | dt: 2909.58ms | tok/sec: 2815.53\n",
            "step   561 | loss: 3.791903 | lr 2.7857e-04 | norm: 0.8754 | dt: 2905.77ms | tok/sec: 2819.22\n",
            "step   562 | loss: 3.584301 | lr 2.7774e-04 | norm: 0.8162 | dt: 2907.71ms | tok/sec: 2817.34\n",
            "step   563 | loss: 3.409811 | lr 2.7691e-04 | norm: 0.7254 | dt: 2912.14ms | tok/sec: 2813.05\n",
            "step   564 | loss: 3.469792 | lr 2.7608e-04 | norm: 0.7208 | dt: 2918.12ms | tok/sec: 2807.29\n",
            "step   565 | loss: 3.320561 | lr 2.7525e-04 | norm: 0.7049 | dt: 2920.60ms | tok/sec: 2804.90\n",
            "step   566 | loss: 3.602757 | lr 2.7442e-04 | norm: 0.8366 | dt: 2898.01ms | tok/sec: 2826.77\n",
            "step   567 | loss: 3.404003 | lr 2.7359e-04 | norm: 0.7263 | dt: 2896.91ms | tok/sec: 2827.84\n",
            "step   568 | loss: 3.370034 | lr 2.7276e-04 | norm: 0.7390 | dt: 2904.88ms | tok/sec: 2820.09\n",
            "step   569 | loss: 3.320089 | lr 2.7193e-04 | norm: 0.6957 | dt: 2923.43ms | tok/sec: 2802.19\n",
            "step   570 | loss: 3.618354 | lr 2.7110e-04 | norm: 0.8677 | dt: 2905.46ms | tok/sec: 2819.51\n",
            "step   571 | loss: 3.308242 | lr 2.7027e-04 | norm: 0.6936 | dt: 2902.73ms | tok/sec: 2822.17\n",
            "step   572 | loss: 3.481740 | lr 2.6945e-04 | norm: 0.7719 | dt: 2901.28ms | tok/sec: 2823.58\n",
            "step   573 | loss: 3.405494 | lr 2.6862e-04 | norm: 0.7363 | dt: 2902.39ms | tok/sec: 2822.50\n",
            "step   574 | loss: 3.640743 | lr 2.6779e-04 | norm: 0.8104 | dt: 2928.97ms | tok/sec: 2796.89\n",
            "step   575 | loss: 3.341865 | lr 2.6697e-04 | norm: 0.7474 | dt: 2912.28ms | tok/sec: 2812.92\n",
            "step   576 | loss: 3.479145 | lr 2.6615e-04 | norm: 0.7545 | dt: 2900.75ms | tok/sec: 2824.09\n",
            "step   577 | loss: 3.464430 | lr 2.6532e-04 | norm: 0.7572 | dt: 2903.54ms | tok/sec: 2821.39\n",
            "step   578 | loss: 3.428969 | lr 2.6450e-04 | norm: 0.7502 | dt: 2916.65ms | tok/sec: 2808.70\n",
            "step   579 | loss: 3.382395 | lr 2.6368e-04 | norm: 0.7213 | dt: 2906.90ms | tok/sec: 2818.12\n",
            "step   580 | loss: 3.289290 | lr 2.6285e-04 | norm: 0.7083 | dt: 2906.01ms | tok/sec: 2818.98\n",
            "step   581 | loss: 3.497844 | lr 2.6203e-04 | norm: 0.8408 | dt: 2910.62ms | tok/sec: 2814.52\n",
            "step   582 | loss: 3.438851 | lr 2.6121e-04 | norm: 0.7339 | dt: 2934.20ms | tok/sec: 2791.90\n",
            "step   583 | loss: 3.421459 | lr 2.6039e-04 | norm: 0.7427 | dt: 2913.07ms | tok/sec: 2812.15\n",
            "step   584 | loss: 3.456634 | lr 2.5957e-04 | norm: 0.7564 | dt: 2906.23ms | tok/sec: 2818.78\n",
            "step   585 | loss: 3.496037 | lr 2.5875e-04 | norm: 0.7929 | dt: 2913.72ms | tok/sec: 2811.52\n",
            "step   586 | loss: 3.367582 | lr 2.5794e-04 | norm: 0.7061 | dt: 2917.51ms | tok/sec: 2807.87\n",
            "step   587 | loss: 3.435361 | lr 2.5712e-04 | norm: 0.7491 | dt: 2916.48ms | tok/sec: 2808.87\n",
            "step   588 | loss: 3.374022 | lr 2.5630e-04 | norm: 0.7709 | dt: 2910.56ms | tok/sec: 2814.57\n",
            "step   589 | loss: 3.436394 | lr 2.5549e-04 | norm: 0.7642 | dt: 2910.08ms | tok/sec: 2815.04\n",
            "step   590 | loss: 3.410442 | lr 2.5467e-04 | norm: 0.7870 | dt: 2914.64ms | tok/sec: 2810.63\n",
            "step   591 | loss: 3.536998 | lr 2.5386e-04 | norm: 0.8825 | dt: 2947.54ms | tok/sec: 2779.27\n",
            "step   592 | loss: 3.271478 | lr 2.5304e-04 | norm: 0.7245 | dt: 2905.33ms | tok/sec: 2819.65\n",
            "step   593 | loss: 3.417642 | lr 2.5223e-04 | norm: 0.7747 | dt: 2909.21ms | tok/sec: 2815.88\n",
            "step   594 | loss: 3.498989 | lr 2.5142e-04 | norm: 0.8011 | dt: 2910.42ms | tok/sec: 2814.71\n",
            "step   595 | loss: 3.420184 | lr 2.5061e-04 | norm: 0.7561 | dt: 2955.78ms | tok/sec: 2771.52\n",
            "step   596 | loss: 3.280109 | lr 2.4980e-04 | norm: 0.6845 | dt: 2904.83ms | tok/sec: 2820.13\n",
            "step   597 | loss: 3.241680 | lr 2.4899e-04 | norm: 0.7201 | dt: 2906.79ms | tok/sec: 2818.23\n",
            "step   598 | loss: 3.237971 | lr 2.4818e-04 | norm: 0.6915 | dt: 2918.49ms | tok/sec: 2806.93\n",
            "step   599 | loss: 3.240258 | lr 2.4737e-04 | norm: 0.6708 | dt: 2914.91ms | tok/sec: 2810.38\n",
            "step   600 | loss: 3.266953 | lr 2.4657e-04 | norm: 0.7051 | dt: 2907.65ms | tok/sec: 2817.40\n",
            "step   601 | loss: 3.378800 | lr 2.4576e-04 | norm: 0.7776 | dt: 2907.52ms | tok/sec: 2817.52\n",
            "step   602 | loss: 3.245221 | lr 2.4495e-04 | norm: 0.7399 | dt: 2904.26ms | tok/sec: 2820.69\n",
            "step   603 | loss: 3.384289 | lr 2.4415e-04 | norm: 0.7960 | dt: 2909.34ms | tok/sec: 2815.76\n",
            "step   604 | loss: 3.420382 | lr 2.4335e-04 | norm: 0.8036 | dt: 2930.75ms | tok/sec: 2795.19\n",
            "step   605 | loss: 3.365525 | lr 2.4254e-04 | norm: 0.7709 | dt: 2905.60ms | tok/sec: 2819.38\n",
            "step   606 | loss: 3.279295 | lr 2.4174e-04 | norm: 0.7458 | dt: 2900.45ms | tok/sec: 2824.39\n",
            "step   607 | loss: 3.249053 | lr 2.4094e-04 | norm: 0.7194 | dt: 2909.84ms | tok/sec: 2815.27\n",
            "step   608 | loss: 3.196761 | lr 2.4014e-04 | norm: 0.6719 | dt: 2920.23ms | tok/sec: 2805.26\n",
            "step   609 | loss: 3.296966 | lr 2.3934e-04 | norm: 0.7304 | dt: 2910.68ms | tok/sec: 2814.46\n",
            "step   610 | loss: 3.222755 | lr 2.3854e-04 | norm: 0.7252 | dt: 2907.36ms | tok/sec: 2817.68\n",
            "step   611 | loss: 3.348320 | lr 2.3774e-04 | norm: 0.7526 | dt: 2897.43ms | tok/sec: 2827.34\n",
            "step   612 | loss: 3.233221 | lr 2.3695e-04 | norm: 0.7095 | dt: 2912.84ms | tok/sec: 2812.37\n",
            "step   613 | loss: 3.308978 | lr 2.3615e-04 | norm: 0.7651 | dt: 2911.57ms | tok/sec: 2813.60\n",
            "step   614 | loss: 3.246606 | lr 2.3536e-04 | norm: 0.7284 | dt: 2905.11ms | tok/sec: 2819.85\n",
            "step   615 | loss: 3.366855 | lr 2.3456e-04 | norm: 0.8108 | dt: 2907.28ms | tok/sec: 2817.75\n",
            "step   616 | loss: 3.229915 | lr 2.3377e-04 | norm: 0.7455 | dt: 2912.49ms | tok/sec: 2812.71\n",
            "step   617 | loss: 3.268491 | lr 2.3298e-04 | norm: 0.7741 | dt: 2905.90ms | tok/sec: 2819.09\n",
            "step   618 | loss: 3.317725 | lr 2.3219e-04 | norm: 0.7643 | dt: 2927.58ms | tok/sec: 2798.21\n",
            "step   619 | loss: 3.328004 | lr 2.3140e-04 | norm: 0.7415 | dt: 2910.81ms | tok/sec: 2814.33\n",
            "step   620 | loss: 3.400679 | lr 2.3061e-04 | norm: 0.8108 | dt: 2930.69ms | tok/sec: 2795.25\n",
            "step   621 | loss: 3.288309 | lr 2.2982e-04 | norm: 0.7464 | dt: 2916.15ms | tok/sec: 2809.18\n",
            "step   622 | loss: 3.200958 | lr 2.2903e-04 | norm: 0.6970 | dt: 2906.97ms | tok/sec: 2818.05\n",
            "step   623 | loss: 3.266758 | lr 2.2824e-04 | norm: 0.6925 | dt: 2906.71ms | tok/sec: 2818.31\n",
            "step   624 | loss: 3.396811 | lr 2.2746e-04 | norm: 0.7888 | dt: 2912.59ms | tok/sec: 2812.62\n",
            "step   625 | loss: 3.346562 | lr 2.2668e-04 | norm: 0.7281 | dt: 2908.82ms | tok/sec: 2816.26\n",
            "step   626 | loss: 3.341364 | lr 2.2589e-04 | norm: 0.7585 | dt: 2914.65ms | tok/sec: 2810.63\n",
            "step   627 | loss: 3.508108 | lr 2.2511e-04 | norm: 0.8540 | dt: 2902.22ms | tok/sec: 2822.66\n",
            "step   628 | loss: 3.196259 | lr 2.2433e-04 | norm: 0.6831 | dt: 2914.70ms | tok/sec: 2810.58\n",
            "step   629 | loss: 3.283392 | lr 2.2355e-04 | norm: 0.7300 | dt: 2921.51ms | tok/sec: 2804.03\n",
            "step   630 | loss: 3.284045 | lr 2.2277e-04 | norm: 0.7243 | dt: 2913.48ms | tok/sec: 2811.76\n",
            "step   631 | loss: 3.342382 | lr 2.2199e-04 | norm: 0.7427 | dt: 2917.02ms | tok/sec: 2808.35\n",
            "step   632 | loss: 3.199801 | lr 2.2122e-04 | norm: 0.7012 | dt: 2908.35ms | tok/sec: 2816.72\n",
            "step   633 | loss: 3.412231 | lr 2.2044e-04 | norm: 0.7993 | dt: 2924.96ms | tok/sec: 2800.72\n",
            "step   634 | loss: 3.264092 | lr 2.1966e-04 | norm: 0.7240 | dt: 2905.90ms | tok/sec: 2819.10\n",
            "step   635 | loss: 3.234812 | lr 2.1889e-04 | norm: 0.7243 | dt: 2910.59ms | tok/sec: 2814.55\n",
            "step   636 | loss: 3.534779 | lr 2.1812e-04 | norm: 0.8350 | dt: 2906.15ms | tok/sec: 2818.85\n",
            "step   637 | loss: 3.297311 | lr 2.1735e-04 | norm: 0.7249 | dt: 2918.83ms | tok/sec: 2806.60\n",
            "step   638 | loss: 3.335438 | lr 2.1658e-04 | norm: 0.7747 | dt: 2915.85ms | tok/sec: 2809.47\n",
            "step   639 | loss: 3.278230 | lr 2.1581e-04 | norm: 0.7423 | dt: 2907.72ms | tok/sec: 2817.33\n",
            "step   640 | loss: 3.086367 | lr 2.1504e-04 | norm: 0.6635 | dt: 2915.63ms | tok/sec: 2809.69\n",
            "step   641 | loss: 3.214253 | lr 2.1427e-04 | norm: 0.7103 | dt: 2928.05ms | tok/sec: 2797.76\n",
            "step   642 | loss: 3.225073 | lr 2.1351e-04 | norm: 0.7161 | dt: 2925.96ms | tok/sec: 2799.76\n",
            "step   643 | loss: 3.339787 | lr 2.1274e-04 | norm: 0.7869 | dt: 2921.48ms | tok/sec: 2804.06\n",
            "step   644 | loss: 3.141097 | lr 2.1198e-04 | norm: 0.6872 | dt: 2902.89ms | tok/sec: 2822.01\n",
            "step   645 | loss: 3.297401 | lr 2.1122e-04 | norm: 0.7593 | dt: 2909.86ms | tok/sec: 2815.26\n",
            "step   646 | loss: 3.322201 | lr 2.1046e-04 | norm: 0.7711 | dt: 2931.83ms | tok/sec: 2794.16\n",
            "step   647 | loss: 3.221816 | lr 2.0970e-04 | norm: 0.7327 | dt: 2922.45ms | tok/sec: 2803.13\n",
            "step   648 | loss: 3.292179 | lr 2.0894e-04 | norm: 0.7665 | dt: 2917.50ms | tok/sec: 2807.88\n",
            "step   649 | loss: 3.148226 | lr 2.0818e-04 | norm: 0.6726 | dt: 2909.06ms | tok/sec: 2816.03\n",
            "step   650 | loss: 3.202951 | lr 2.0742e-04 | norm: 0.6984 | dt: 2926.08ms | tok/sec: 2799.65\n",
            "step   651 | loss: 3.170753 | lr 2.0667e-04 | norm: 0.7024 | dt: 2899.91ms | tok/sec: 2824.91\n",
            "step   652 | loss: 3.292200 | lr 2.0591e-04 | norm: 0.7668 | dt: 2917.46ms | tok/sec: 2807.92\n",
            "step   653 | loss: 3.218349 | lr 2.0516e-04 | norm: 0.7293 | dt: 2907.32ms | tok/sec: 2817.71\n",
            "step   654 | loss: 3.197924 | lr 2.0441e-04 | norm: 0.7238 | dt: 2933.09ms | tok/sec: 2792.96\n",
            "step   655 | loss: 3.238067 | lr 2.0366e-04 | norm: 0.7407 | dt: 2917.77ms | tok/sec: 2807.63\n",
            "step   656 | loss: 3.261680 | lr 2.0291e-04 | norm: 0.7631 | dt: 2906.66ms | tok/sec: 2818.36\n",
            "step   657 | loss: 3.222229 | lr 2.0216e-04 | norm: 0.7354 | dt: 2908.48ms | tok/sec: 2816.60\n",
            "step   658 | loss: 3.169492 | lr 2.0142e-04 | norm: 0.7264 | dt: 2913.33ms | tok/sec: 2811.90\n",
            "step   659 | loss: 3.192301 | lr 2.0067e-04 | norm: 0.6944 | dt: 2918.41ms | tok/sec: 2807.01\n",
            "step   660 | loss: 3.232467 | lr 1.9993e-04 | norm: 0.7548 | dt: 2922.77ms | tok/sec: 2802.82\n",
            "step   661 | loss: 3.224063 | lr 1.9918e-04 | norm: 0.7459 | dt: 2899.84ms | tok/sec: 2824.98\n",
            "step   662 | loss: 3.267186 | lr 1.9844e-04 | norm: 0.7938 | dt: 2902.94ms | tok/sec: 2821.96\n",
            "step   663 | loss: 3.129763 | lr 1.9770e-04 | norm: 0.7077 | dt: 2928.31ms | tok/sec: 2797.52\n",
            "step   664 | loss: 3.236350 | lr 1.9696e-04 | norm: 0.7295 | dt: 2909.81ms | tok/sec: 2815.31\n",
            "step   665 | loss: 3.261230 | lr 1.9623e-04 | norm: 0.7782 | dt: 2904.54ms | tok/sec: 2820.41\n",
            "step   666 | loss: 3.264563 | lr 1.9549e-04 | norm: 0.7765 | dt: 2908.91ms | tok/sec: 2816.17\n",
            "step   667 | loss: 3.060992 | lr 1.9476e-04 | norm: 0.6573 | dt: 2910.18ms | tok/sec: 2814.95\n",
            "step   668 | loss: 3.240288 | lr 1.9402e-04 | norm: 0.7372 | dt: 2913.45ms | tok/sec: 2811.78\n",
            "step   669 | loss: 3.245235 | lr 1.9329e-04 | norm: 0.7344 | dt: 2905.57ms | tok/sec: 2819.41\n",
            "step   670 | loss: 3.259094 | lr 1.9256e-04 | norm: 0.7623 | dt: 2898.97ms | tok/sec: 2825.83\n",
            "step   671 | loss: 3.309526 | lr 1.9183e-04 | norm: 0.7647 | dt: 2916.23ms | tok/sec: 2809.10\n",
            "step   672 | loss: 3.198034 | lr 1.9110e-04 | norm: 0.7251 | dt: 2956.93ms | tok/sec: 2770.44\n",
            "step   673 | loss: 3.126434 | lr 1.9037e-04 | norm: 0.6913 | dt: 2901.73ms | tok/sec: 2823.14\n",
            "step   674 | loss: 3.205962 | lr 1.8965e-04 | norm: 0.7269 | dt: 2904.39ms | tok/sec: 2820.56\n",
            "step   675 | loss: 3.214440 | lr 1.8893e-04 | norm: 0.7135 | dt: 2902.51ms | tok/sec: 2822.39\n",
            "step   676 | loss: 3.248664 | lr 1.8820e-04 | norm: 0.7546 | dt: 2913.66ms | tok/sec: 2811.58\n",
            "step   677 | loss: 3.194684 | lr 1.8748e-04 | norm: 0.7523 | dt: 2901.37ms | tok/sec: 2823.49\n",
            "step   678 | loss: 3.276680 | lr 1.8676e-04 | norm: 0.7521 | dt: 2912.85ms | tok/sec: 2812.36\n",
            "step   679 | loss: 3.379221 | lr 1.8604e-04 | norm: 0.8142 | dt: 2907.66ms | tok/sec: 2817.38\n",
            "step   680 | loss: 3.247704 | lr 1.8533e-04 | norm: 0.7395 | dt: 2926.11ms | tok/sec: 2799.63\n",
            "step   681 | loss: 3.354172 | lr 1.8461e-04 | norm: 0.8179 | dt: 2907.75ms | tok/sec: 2817.30\n",
            "step   682 | loss: 3.241664 | lr 1.8390e-04 | norm: 0.7489 | dt: 2906.58ms | tok/sec: 2818.43\n",
            "step   683 | loss: 3.106759 | lr 1.8318e-04 | norm: 0.6502 | dt: 2909.20ms | tok/sec: 2815.89\n",
            "step   684 | loss: 3.180748 | lr 1.8247e-04 | norm: 0.7218 | dt: 2913.14ms | tok/sec: 2812.09\n",
            "step   685 | loss: 3.275483 | lr 1.8176e-04 | norm: 0.7567 | dt: 2910.02ms | tok/sec: 2815.10\n",
            "step   686 | loss: 3.109370 | lr 1.8106e-04 | norm: 0.6839 | dt: 2911.05ms | tok/sec: 2814.10\n",
            "step   687 | loss: 3.227769 | lr 1.8035e-04 | norm: 0.7100 | dt: 2912.42ms | tok/sec: 2812.78\n",
            "step   688 | loss: 3.140622 | lr 1.7964e-04 | norm: 0.7158 | dt: 2899.57ms | tok/sec: 2825.24\n",
            "step   689 | loss: 3.158514 | lr 1.7894e-04 | norm: 0.7241 | dt: 2915.99ms | tok/sec: 2809.34\n",
            "step   690 | loss: 3.064967 | lr 1.7824e-04 | norm: 0.6706 | dt: 2903.27ms | tok/sec: 2821.65\n",
            "step   691 | loss: 3.127393 | lr 1.7754e-04 | norm: 0.7322 | dt: 2926.61ms | tok/sec: 2799.14\n",
            "step   692 | loss: 3.269585 | lr 1.7684e-04 | norm: 0.8178 | dt: 2926.66ms | tok/sec: 2799.09\n",
            "step   693 | loss: 3.308641 | lr 1.7614e-04 | norm: 0.8142 | dt: 2910.14ms | tok/sec: 2814.98\n",
            "step   694 | loss: 3.096525 | lr 1.7544e-04 | norm: 0.7176 | dt: 2912.68ms | tok/sec: 2812.53\n",
            "step   695 | loss: 3.303977 | lr 1.7475e-04 | norm: 0.7773 | dt: 2908.11ms | tok/sec: 2816.95\n",
            "step   696 | loss: 3.118846 | lr 1.7406e-04 | norm: 0.6794 | dt: 2906.22ms | tok/sec: 2818.78\n",
            "step   697 | loss: 3.178853 | lr 1.7336e-04 | norm: 0.7679 | dt: 2935.52ms | tok/sec: 2790.65\n",
            "step   698 | loss: 3.218780 | lr 1.7267e-04 | norm: 0.7304 | dt: 2922.02ms | tok/sec: 2803.54\n",
            "step   699 | loss: 2.986212 | lr 1.7198e-04 | norm: 0.6171 | dt: 2908.50ms | tok/sec: 2816.58\n",
            "step   700 | loss: 3.381891 | lr 1.7130e-04 | norm: 0.7904 | dt: 2907.60ms | tok/sec: 2817.45\n",
            "step   701 | loss: 3.278988 | lr 1.7061e-04 | norm: 0.7844 | dt: 2930.70ms | tok/sec: 2795.24\n",
            "step   702 | loss: 3.235348 | lr 1.6993e-04 | norm: 0.7463 | dt: 2912.98ms | tok/sec: 2812.24\n",
            "step   703 | loss: 3.331911 | lr 1.6925e-04 | norm: 0.7970 | dt: 2918.59ms | tok/sec: 2806.83\n",
            "step   704 | loss: 3.373724 | lr 1.6857e-04 | norm: 0.7883 | dt: 2909.59ms | tok/sec: 2815.52\n",
            "step   705 | loss: 3.104007 | lr 1.6789e-04 | norm: 0.6998 | dt: 2907.99ms | tok/sec: 2817.06\n",
            "step   706 | loss: 3.184258 | lr 1.6721e-04 | norm: 0.7290 | dt: 2934.13ms | tok/sec: 2791.97\n",
            "step   707 | loss: 3.120095 | lr 1.6653e-04 | norm: 0.6966 | dt: 2935.16ms | tok/sec: 2790.99\n",
            "step   708 | loss: 3.169192 | lr 1.6586e-04 | norm: 0.8418 | dt: 2901.85ms | tok/sec: 2823.03\n",
            "step   709 | loss: 3.315136 | lr 1.6519e-04 | norm: 0.7857 | dt: 2915.89ms | tok/sec: 2809.43\n",
            "step   710 | loss: 3.145683 | lr 1.6452e-04 | norm: 0.7058 | dt: 2925.43ms | tok/sec: 2800.27\n",
            "step   711 | loss: 3.122316 | lr 1.6385e-04 | norm: 0.7158 | dt: 2909.61ms | tok/sec: 2815.50\n",
            "step   712 | loss: 3.014621 | lr 1.6318e-04 | norm: 0.6983 | dt: 2908.69ms | tok/sec: 2816.39\n",
            "step   713 | loss: 3.159233 | lr 1.6251e-04 | norm: 0.7311 | dt: 2914.37ms | tok/sec: 2810.90\n",
            "step   714 | loss: 3.198467 | lr 1.6185e-04 | norm: 0.7989 | dt: 2934.46ms | tok/sec: 2791.66\n",
            "step   715 | loss: 3.234018 | lr 1.6118e-04 | norm: 0.7552 | dt: 2907.37ms | tok/sec: 2817.67\n",
            "step   716 | loss: 3.084816 | lr 1.6052e-04 | norm: 0.6859 | dt: 2931.72ms | tok/sec: 2794.26\n",
            "step   717 | loss: 3.318159 | lr 1.5986e-04 | norm: 0.8350 | dt: 2908.53ms | tok/sec: 2816.54\n",
            "step   718 | loss: 3.255900 | lr 1.5921e-04 | norm: 0.7564 | dt: 2913.12ms | tok/sec: 2812.10\n",
            "step   719 | loss: 3.055178 | lr 1.5855e-04 | norm: 0.6696 | dt: 2908.42ms | tok/sec: 2816.65\n",
            "step   720 | loss: 3.046631 | lr 1.5790e-04 | norm: 0.6828 | dt: 2909.46ms | tok/sec: 2815.65\n",
            "step   721 | loss: 2.993597 | lr 1.5724e-04 | norm: 0.6435 | dt: 2910.91ms | tok/sec: 2814.24\n",
            "step   722 | loss: 3.253909 | lr 1.5659e-04 | norm: 0.7718 | dt: 2923.45ms | tok/sec: 2802.17\n",
            "step   723 | loss: 3.066536 | lr 1.5594e-04 | norm: 0.6733 | dt: 2929.68ms | tok/sec: 2796.21\n",
            "step   724 | loss: 3.128239 | lr 1.5529e-04 | norm: 0.6875 | dt: 2913.11ms | tok/sec: 2812.11\n",
            "step   725 | loss: 3.071164 | lr 1.5465e-04 | norm: 0.6755 | dt: 2910.50ms | tok/sec: 2814.64\n",
            "step   726 | loss: 3.273637 | lr 1.5400e-04 | norm: 0.7988 | dt: 2910.76ms | tok/sec: 2814.38\n",
            "step   727 | loss: 3.175481 | lr 1.5336e-04 | norm: 0.7602 | dt: 2918.01ms | tok/sec: 2807.39\n",
            "step   728 | loss: 3.240162 | lr 1.5272e-04 | norm: 0.7845 | dt: 2903.57ms | tok/sec: 2821.36\n",
            "step   729 | loss: 3.115170 | lr 1.5208e-04 | norm: 0.7317 | dt: 2908.54ms | tok/sec: 2816.54\n",
            "step   730 | loss: 3.083918 | lr 1.5145e-04 | norm: 0.6846 | dt: 2908.81ms | tok/sec: 2816.27\n",
            "step   731 | loss: 2.960563 | lr 1.5081e-04 | norm: 0.6295 | dt: 2936.41ms | tok/sec: 2789.80\n",
            "step   732 | loss: 3.168110 | lr 1.5018e-04 | norm: 0.7430 | dt: 2908.20ms | tok/sec: 2816.86\n",
            "step   733 | loss: 3.114017 | lr 1.4954e-04 | norm: 0.7389 | dt: 2910.63ms | tok/sec: 2814.51\n",
            "step   734 | loss: 3.113704 | lr 1.4891e-04 | norm: 0.7336 | dt: 2912.56ms | tok/sec: 2812.64\n",
            "step   735 | loss: 3.179901 | lr 1.4829e-04 | norm: 0.7557 | dt: 2925.58ms | tok/sec: 2800.13\n",
            "step   736 | loss: 3.111112 | lr 1.4766e-04 | norm: 0.7733 | dt: 2951.39ms | tok/sec: 2775.64\n",
            "step   737 | loss: 3.051083 | lr 1.4704e-04 | norm: 0.6746 | dt: 2910.95ms | tok/sec: 2814.20\n",
            "step   738 | loss: 3.097813 | lr 1.4641e-04 | norm: 0.7305 | dt: 2907.75ms | tok/sec: 2817.30\n",
            "step   739 | loss: 3.096164 | lr 1.4579e-04 | norm: 0.6961 | dt: 2912.29ms | tok/sec: 2812.91\n",
            "step   740 | loss: 3.167035 | lr 1.4517e-04 | norm: 0.7384 | dt: 2929.71ms | tok/sec: 2796.18\n",
            "step   741 | loss: 3.166340 | lr 1.4455e-04 | norm: 0.7219 | dt: 2894.48ms | tok/sec: 2830.21\n",
            "step   742 | loss: 3.139533 | lr 1.4394e-04 | norm: 0.7115 | dt: 2904.17ms | tok/sec: 2820.77\n",
            "step   743 | loss: 3.207995 | lr 1.4333e-04 | norm: 0.7820 | dt: 2921.27ms | tok/sec: 2804.26\n",
            "step   744 | loss: 3.065670 | lr 1.4271e-04 | norm: 0.7053 | dt: 2922.74ms | tok/sec: 2802.85\n",
            "step   745 | loss: 3.083164 | lr 1.4210e-04 | norm: 0.7053 | dt: 2932.79ms | tok/sec: 2793.25\n",
            "step   746 | loss: 3.255963 | lr 1.4150e-04 | norm: 0.8089 | dt: 2905.19ms | tok/sec: 2819.78\n",
            "step   747 | loss: 3.073445 | lr 1.4089e-04 | norm: 0.6982 | dt: 2907.55ms | tok/sec: 2817.49\n",
            "step   748 | loss: 3.128046 | lr 1.4028e-04 | norm: 0.7569 | dt: 2907.99ms | tok/sec: 2817.07\n",
            "step   749 | loss: 2.995146 | lr 1.3968e-04 | norm: 0.6293 | dt: 2917.93ms | tok/sec: 2807.47\n",
            "step   750 | loss: 3.090468 | lr 1.3908e-04 | norm: 0.6997 | dt: 24137.78ms | tok/sec: 339.38\n",
            "step   751 | loss: 3.134497 | lr 1.3848e-04 | norm: 0.7091 | dt: 2858.07ms | tok/sec: 2866.27\n",
            "step   752 | loss: 3.125198 | lr 1.3789e-04 | norm: 0.7365 | dt: 2871.01ms | tok/sec: 2853.35\n",
            "step   753 | loss: 3.062510 | lr 1.3729e-04 | norm: 0.7126 | dt: 2928.42ms | tok/sec: 2797.42\n",
            "step   754 | loss: 2.975622 | lr 1.3670e-04 | norm: 0.6390 | dt: 2966.02ms | tok/sec: 2761.95\n",
            "step   755 | loss: 3.023837 | lr 1.3611e-04 | norm: 0.7067 | dt: 2950.77ms | tok/sec: 2776.22\n",
            "step   756 | loss: 2.920919 | lr 1.3552e-04 | norm: 0.6093 | dt: 2972.08ms | tok/sec: 2756.32\n",
            "step   757 | loss: 2.985817 | lr 1.3493e-04 | norm: 0.6643 | dt: 2992.55ms | tok/sec: 2737.47\n",
            "step   758 | loss: 3.005166 | lr 1.3434e-04 | norm: 0.6750 | dt: 3012.89ms | tok/sec: 2718.99\n",
            "step   759 | loss: 3.156703 | lr 1.3376e-04 | norm: 0.7790 | dt: 3023.31ms | tok/sec: 2709.61\n",
            "step   760 | loss: 2.961890 | lr 1.3318e-04 | norm: 0.6486 | dt: 3016.10ms | tok/sec: 2716.09\n",
            "step   761 | loss: 3.117947 | lr 1.3260e-04 | norm: 0.7468 | dt: 2987.84ms | tok/sec: 2741.78\n",
            "step   762 | loss: 3.154242 | lr 1.3202e-04 | norm: 0.7677 | dt: 3006.49ms | tok/sec: 2724.77\n",
            "step   763 | loss: 2.954538 | lr 1.3145e-04 | norm: 0.6373 | dt: 2976.77ms | tok/sec: 2751.97\n",
            "step   764 | loss: 2.923943 | lr 1.3087e-04 | norm: 0.6122 | dt: 2959.27ms | tok/sec: 2768.25\n",
            "step   765 | loss: 3.116493 | lr 1.3030e-04 | norm: 0.7143 | dt: 2931.07ms | tok/sec: 2794.88\n",
            "step   766 | loss: 2.860860 | lr 1.2973e-04 | norm: 0.5636 | dt: 2921.25ms | tok/sec: 2804.28\n",
            "step   767 | loss: 2.978529 | lr 1.2916e-04 | norm: 0.6824 | dt: 2917.65ms | tok/sec: 2807.74\n",
            "step   768 | loss: 3.082665 | lr 1.2860e-04 | norm: 0.6998 | dt: 2898.60ms | tok/sec: 2826.19\n",
            "step   769 | loss: 3.254371 | lr 1.2803e-04 | norm: 0.8621 | dt: 2885.65ms | tok/sec: 2838.88\n",
            "step   770 | loss: 2.931544 | lr 1.2747e-04 | norm: 0.6576 | dt: 2882.98ms | tok/sec: 2841.51\n",
            "step   771 | loss: 3.133486 | lr 1.2691e-04 | norm: 0.7903 | dt: 2904.41ms | tok/sec: 2820.54\n",
            "step   772 | loss: 3.137619 | lr 1.2635e-04 | norm: 0.7541 | dt: 2881.95ms | tok/sec: 2842.52\n",
            "step   773 | loss: 3.080072 | lr 1.2580e-04 | norm: 0.7413 | dt: 2882.24ms | tok/sec: 2842.24\n",
            "step   774 | loss: 3.000553 | lr 1.2524e-04 | norm: 0.6565 | dt: 2871.50ms | tok/sec: 2852.86\n",
            "step   775 | loss: 3.084973 | lr 1.2469e-04 | norm: 0.6928 | dt: 2899.40ms | tok/sec: 2825.42\n",
            "step   776 | loss: 2.988839 | lr 1.2414e-04 | norm: 0.6697 | dt: 2877.56ms | tok/sec: 2846.85\n",
            "step   777 | loss: 3.047475 | lr 1.2359e-04 | norm: 0.6876 | dt: 2868.94ms | tok/sec: 2855.41\n",
            "step   778 | loss: 3.002212 | lr 1.2305e-04 | norm: 0.6598 | dt: 2874.59ms | tok/sec: 2849.79\n",
            "step   779 | loss: 3.045869 | lr 1.2250e-04 | norm: 0.6980 | dt: 2895.64ms | tok/sec: 2829.08\n",
            "step   780 | loss: 3.027925 | lr 1.2196e-04 | norm: 0.6819 | dt: 2905.92ms | tok/sec: 2819.07\n",
            "step   781 | loss: 3.132627 | lr 1.2142e-04 | norm: 0.7666 | dt: 2892.00ms | tok/sec: 2832.64\n",
            "step   782 | loss: 3.018189 | lr 1.2088e-04 | norm: 0.6670 | dt: 2894.07ms | tok/sec: 2830.61\n",
            "step   783 | loss: 3.174231 | lr 1.2035e-04 | norm: 0.7944 | dt: 2904.03ms | tok/sec: 2820.91\n",
            "step   784 | loss: 3.001916 | lr 1.1982e-04 | norm: 0.6803 | dt: 2918.96ms | tok/sec: 2806.48\n",
            "step   785 | loss: 3.068888 | lr 1.1928e-04 | norm: 0.7342 | dt: 2909.77ms | tok/sec: 2815.35\n",
            "step   786 | loss: 2.946553 | lr 1.1875e-04 | norm: 0.6422 | dt: 2915.16ms | tok/sec: 2810.14\n",
            "step   787 | loss: 3.094418 | lr 1.1823e-04 | norm: 0.7260 | dt: 2916.63ms | tok/sec: 2808.72\n",
            "step   788 | loss: 2.977402 | lr 1.1770e-04 | norm: 0.6483 | dt: 2929.46ms | tok/sec: 2796.42\n",
            "step   789 | loss: 2.938901 | lr 1.1718e-04 | norm: 0.6400 | dt: 2923.82ms | tok/sec: 2801.82\n",
            "step   790 | loss: 3.059032 | lr 1.1666e-04 | norm: 0.6716 | dt: 2929.55ms | tok/sec: 2796.33\n",
            "step   791 | loss: 3.056059 | lr 1.1614e-04 | norm: 0.7176 | dt: 2929.77ms | tok/sec: 2796.12\n",
            "step   792 | loss: 2.908499 | lr 1.1562e-04 | norm: 0.6485 | dt: 2946.64ms | tok/sec: 2780.11\n",
            "step   793 | loss: 3.102739 | lr 1.1511e-04 | norm: 0.7798 | dt: 2936.12ms | tok/sec: 2790.08\n",
            "step   794 | loss: 2.940820 | lr 1.1460e-04 | norm: 0.6455 | dt: 2932.79ms | tok/sec: 2793.25\n",
            "step   795 | loss: 3.129652 | lr 1.1409e-04 | norm: 0.7544 | dt: 2926.09ms | tok/sec: 2799.64\n",
            "step   796 | loss: 3.004066 | lr 1.1358e-04 | norm: 0.7028 | dt: 2924.81ms | tok/sec: 2800.87\n",
            "step   797 | loss: 2.990232 | lr 1.1307e-04 | norm: 0.6768 | dt: 2909.35ms | tok/sec: 2815.75\n",
            "step   798 | loss: 3.075917 | lr 1.1257e-04 | norm: 0.6992 | dt: 2912.71ms | tok/sec: 2812.50\n",
            "step   799 | loss: 2.911435 | lr 1.1207e-04 | norm: 0.6260 | dt: 2925.28ms | tok/sec: 2800.42\n",
            "step   800 | loss: 2.901581 | lr 1.1157e-04 | norm: 0.6128 | dt: 2901.44ms | tok/sec: 2823.42\n",
            "step   801 | loss: 3.041849 | lr 1.1107e-04 | norm: 0.7202 | dt: 2939.53ms | tok/sec: 2786.84\n",
            "step   802 | loss: 2.990934 | lr 1.1057e-04 | norm: 0.6493 | dt: 2899.91ms | tok/sec: 2824.92\n",
            "step   803 | loss: 3.028911 | lr 1.1008e-04 | norm: 0.7004 | dt: 2899.11ms | tok/sec: 2825.69\n",
            "step   804 | loss: 3.127317 | lr 1.0959e-04 | norm: 0.7710 | dt: 2900.21ms | tok/sec: 2824.62\n",
            "step   805 | loss: 2.903996 | lr 1.0910e-04 | norm: 0.6005 | dt: 2897.46ms | tok/sec: 2827.30\n",
            "step   806 | loss: 2.987124 | lr 1.0861e-04 | norm: 0.6683 | dt: 2894.06ms | tok/sec: 2830.62\n",
            "step   807 | loss: 3.052213 | lr 1.0813e-04 | norm: 0.7261 | dt: 2899.40ms | tok/sec: 2825.41\n",
            "step   808 | loss: 2.954805 | lr 1.0765e-04 | norm: 0.6690 | dt: 2903.43ms | tok/sec: 2821.49\n",
            "step   809 | loss: 2.872534 | lr 1.0717e-04 | norm: 0.5715 | dt: 2916.05ms | tok/sec: 2809.28\n",
            "step   810 | loss: 2.941091 | lr 1.0669e-04 | norm: 0.6597 | dt: 2900.57ms | tok/sec: 2824.28\n",
            "step   811 | loss: 3.068902 | lr 1.0621e-04 | norm: 0.7128 | dt: 2897.65ms | tok/sec: 2827.12\n",
            "step   812 | loss: 3.053768 | lr 1.0574e-04 | norm: 0.7223 | dt: 2901.60ms | tok/sec: 2823.27\n",
            "step   813 | loss: 2.917946 | lr 1.0527e-04 | norm: 0.6222 | dt: 2909.98ms | tok/sec: 2815.14\n",
            "step   814 | loss: 3.028826 | lr 1.0480e-04 | norm: 0.7190 | dt: 2907.76ms | tok/sec: 2817.29\n",
            "step   815 | loss: 3.125937 | lr 1.0433e-04 | norm: 0.7473 | dt: 2900.44ms | tok/sec: 2824.40\n",
            "step   816 | loss: 2.965856 | lr 1.0387e-04 | norm: 0.7025 | dt: 2905.56ms | tok/sec: 2819.42\n",
            "step   817 | loss: 3.104008 | lr 1.0341e-04 | norm: 0.7533 | dt: 2914.24ms | tok/sec: 2811.03\n",
            "step   818 | loss: 2.952441 | lr 1.0294e-04 | norm: 0.6615 | dt: 2911.56ms | tok/sec: 2813.61\n",
            "step   819 | loss: 3.044123 | lr 1.0249e-04 | norm: 0.7126 | dt: 2910.19ms | tok/sec: 2814.93\n",
            "step   820 | loss: 2.984449 | lr 1.0203e-04 | norm: 0.6661 | dt: 2911.62ms | tok/sec: 2813.55\n",
            "step   821 | loss: 3.057430 | lr 1.0158e-04 | norm: 0.7379 | dt: 2922.12ms | tok/sec: 2803.45\n",
            "step   822 | loss: 3.014018 | lr 1.0113e-04 | norm: 0.6989 | dt: 2918.84ms | tok/sec: 2806.59\n",
            "step   823 | loss: 2.961976 | lr 1.0068e-04 | norm: 0.6558 | dt: 2905.86ms | tok/sec: 2819.13\n",
            "step   824 | loss: 2.931203 | lr 1.0023e-04 | norm: 0.6622 | dt: 2910.10ms | tok/sec: 2815.02\n",
            "step   825 | loss: 2.965641 | lr 9.9787e-05 | norm: 0.6751 | dt: 2911.54ms | tok/sec: 2813.63\n",
            "step   826 | loss: 2.991548 | lr 9.9345e-05 | norm: 0.6874 | dt: 2927.01ms | tok/sec: 2798.76\n",
            "step   827 | loss: 2.935945 | lr 9.8905e-05 | norm: 0.6639 | dt: 2926.31ms | tok/sec: 2799.43\n",
            "step   828 | loss: 2.975655 | lr 9.8468e-05 | norm: 0.7066 | dt: 2917.51ms | tok/sec: 2807.87\n",
            "step   829 | loss: 2.916207 | lr 9.8033e-05 | norm: 0.6397 | dt: 2919.96ms | tok/sec: 2805.51\n",
            "step   830 | loss: 2.854180 | lr 9.7600e-05 | norm: 0.6146 | dt: 2911.64ms | tok/sec: 2813.54\n",
            "step   831 | loss: 3.031546 | lr 9.7169e-05 | norm: 0.7278 | dt: 2914.67ms | tok/sec: 2810.61\n",
            "step   832 | loss: 2.983087 | lr 9.6741e-05 | norm: 0.6633 | dt: 2901.83ms | tok/sec: 2823.04\n",
            "step   833 | loss: 3.006981 | lr 9.6315e-05 | norm: 0.6871 | dt: 2931.55ms | tok/sec: 2794.43\n",
            "step   834 | loss: 3.121514 | lr 9.5891e-05 | norm: 0.7432 | dt: 2916.73ms | tok/sec: 2808.62\n",
            "step   835 | loss: 3.072378 | lr 9.5469e-05 | norm: 0.7627 | dt: 2931.39ms | tok/sec: 2794.58\n",
            "step   836 | loss: 2.984050 | lr 9.5050e-05 | norm: 0.7028 | dt: 2910.48ms | tok/sec: 2814.65\n",
            "step   837 | loss: 2.933082 | lr 9.4634e-05 | norm: 0.6394 | dt: 2908.95ms | tok/sec: 2816.13\n",
            "step   838 | loss: 2.994665 | lr 9.4219e-05 | norm: 0.6989 | dt: 2922.97ms | tok/sec: 2802.63\n",
            "step   839 | loss: 2.880050 | lr 9.3807e-05 | norm: 0.6362 | dt: 2940.00ms | tok/sec: 2786.40\n",
            "step   840 | loss: 2.866824 | lr 9.3397e-05 | norm: 0.6041 | dt: 2910.09ms | tok/sec: 2815.03\n",
            "step   841 | loss: 2.917142 | lr 9.2990e-05 | norm: 0.6434 | dt: 2902.51ms | tok/sec: 2822.38\n",
            "step   842 | loss: 2.950604 | lr 9.2585e-05 | norm: 0.6823 | dt: 2916.41ms | tok/sec: 2808.93\n",
            "step   843 | loss: 3.011753 | lr 9.2182e-05 | norm: 0.6982 | dt: 2922.74ms | tok/sec: 2802.85\n",
            "step   844 | loss: 2.982486 | lr 9.1781e-05 | norm: 0.7451 | dt: 2915.11ms | tok/sec: 2810.18\n",
            "step   845 | loss: 2.996470 | lr 9.1383e-05 | norm: 0.7196 | dt: 2907.66ms | tok/sec: 2817.38\n",
            "step   846 | loss: 2.984011 | lr 9.0988e-05 | norm: 0.6744 | dt: 2905.45ms | tok/sec: 2819.53\n",
            "step   847 | loss: 2.923613 | lr 9.0594e-05 | norm: 0.6586 | dt: 2930.64ms | tok/sec: 2795.30\n",
            "step   848 | loss: 3.099270 | lr 9.0203e-05 | norm: 0.7492 | dt: 2928.07ms | tok/sec: 2797.75\n",
            "step   849 | loss: 2.947551 | lr 8.9815e-05 | norm: 0.6886 | dt: 2911.09ms | tok/sec: 2814.07\n",
            "step   850 | loss: 2.930449 | lr 8.9428e-05 | norm: 0.7018 | dt: 2914.03ms | tok/sec: 2811.23\n",
            "step   851 | loss: 2.980808 | lr 8.9044e-05 | norm: 0.6839 | dt: 2916.12ms | tok/sec: 2809.22\n",
            "step   852 | loss: 2.862674 | lr 8.8663e-05 | norm: 0.6105 | dt: 2922.30ms | tok/sec: 2803.27\n",
            "step   853 | loss: 2.928890 | lr 8.8284e-05 | norm: 0.6604 | dt: 2912.10ms | tok/sec: 2813.09\n",
            "step   854 | loss: 3.030537 | lr 8.7907e-05 | norm: 0.7295 | dt: 2910.98ms | tok/sec: 2814.17\n",
            "step   855 | loss: 2.955305 | lr 8.7533e-05 | norm: 0.6613 | dt: 2909.02ms | tok/sec: 2816.07\n",
            "step   856 | loss: 2.862423 | lr 8.7161e-05 | norm: 0.6356 | dt: 2926.35ms | tok/sec: 2799.39\n",
            "step   857 | loss: 2.887313 | lr 8.6791e-05 | norm: 0.7157 | dt: 2912.44ms | tok/sec: 2812.76\n",
            "step   858 | loss: 2.867100 | lr 8.6424e-05 | norm: 0.6278 | dt: 2909.30ms | tok/sec: 2815.79\n",
            "step   859 | loss: 2.947769 | lr 8.6059e-05 | norm: 0.6697 | dt: 2904.60ms | tok/sec: 2820.36\n",
            "step   860 | loss: 3.048453 | lr 8.5697e-05 | norm: 0.7747 | dt: 2921.31ms | tok/sec: 2804.22\n",
            "step   861 | loss: 2.862885 | lr 8.5337e-05 | norm: 0.6398 | dt: 2929.75ms | tok/sec: 2796.14\n",
            "step   862 | loss: 2.926916 | lr 8.4979e-05 | norm: 0.6693 | dt: 2897.89ms | tok/sec: 2826.89\n",
            "step   863 | loss: 2.872936 | lr 8.4624e-05 | norm: 0.6278 | dt: 2904.07ms | tok/sec: 2820.87\n",
            "step   864 | loss: 2.966716 | lr 8.4271e-05 | norm: 0.6958 | dt: 2907.46ms | tok/sec: 2817.58\n",
            "step   865 | loss: 2.860157 | lr 8.3921e-05 | norm: 0.6104 | dt: 2898.67ms | tok/sec: 2826.12\n",
            "step   866 | loss: 3.061816 | lr 8.3573e-05 | norm: 0.7881 | dt: 2903.46ms | tok/sec: 2821.46\n",
            "step   867 | loss: 2.956127 | lr 8.3228e-05 | norm: 0.6787 | dt: 2915.24ms | tok/sec: 2810.06\n",
            "step   868 | loss: 2.868446 | lr 8.2885e-05 | norm: 0.6108 | dt: 2905.49ms | tok/sec: 2819.49\n",
            "step   869 | loss: 2.942531 | lr 8.2544e-05 | norm: 0.6669 | dt: 2904.75ms | tok/sec: 2820.21\n",
            "step   870 | loss: 2.885520 | lr 8.2206e-05 | norm: 0.6166 | dt: 2903.25ms | tok/sec: 2821.66\n",
            "step   871 | loss: 2.832293 | lr 8.1871e-05 | norm: 0.5995 | dt: 2901.47ms | tok/sec: 2823.39\n",
            "step   872 | loss: 2.923256 | lr 8.1537e-05 | norm: 0.6655 | dt: 2910.78ms | tok/sec: 2814.36\n",
            "step   873 | loss: 2.912423 | lr 8.1207e-05 | norm: 0.6764 | dt: 2916.25ms | tok/sec: 2809.09\n",
            "step   874 | loss: 2.857221 | lr 8.0878e-05 | norm: 0.6116 | dt: 2897.64ms | tok/sec: 2827.13\n",
            "step   875 | loss: 2.812541 | lr 8.0553e-05 | norm: 0.6044 | dt: 2913.55ms | tok/sec: 2811.69\n",
            "step   876 | loss: 2.953890 | lr 8.0229e-05 | norm: 0.6656 | dt: 2908.31ms | tok/sec: 2816.76\n",
            "step   877 | loss: 2.921178 | lr 7.9908e-05 | norm: 0.6603 | dt: 2928.12ms | tok/sec: 2797.70\n",
            "step   878 | loss: 2.857705 | lr 7.9590e-05 | norm: 0.6146 | dt: 2904.12ms | tok/sec: 2820.82\n",
            "step   879 | loss: 2.875416 | lr 7.9274e-05 | norm: 0.6088 | dt: 2903.93ms | tok/sec: 2821.00\n",
            "step   880 | loss: 2.988428 | lr 7.8960e-05 | norm: 0.7084 | dt: 2905.24ms | tok/sec: 2819.73\n",
            "step   881 | loss: 2.935866 | lr 7.8649e-05 | norm: 0.6720 | dt: 2934.87ms | tok/sec: 2791.27\n",
            "step   882 | loss: 2.906542 | lr 7.8341e-05 | norm: 0.6450 | dt: 2903.17ms | tok/sec: 2821.75\n",
            "step   883 | loss: 2.921776 | lr 7.8035e-05 | norm: 0.6753 | dt: 2902.76ms | tok/sec: 2822.14\n",
            "step   884 | loss: 2.839829 | lr 7.7731e-05 | norm: 0.6173 | dt: 2906.34ms | tok/sec: 2818.67\n",
            "step   885 | loss: 2.967997 | lr 7.7430e-05 | norm: 0.6619 | dt: 2909.94ms | tok/sec: 2815.18\n",
            "step   886 | loss: 2.935197 | lr 7.7132e-05 | norm: 0.6730 | dt: 2913.32ms | tok/sec: 2811.91\n",
            "step   887 | loss: 2.977559 | lr 7.6835e-05 | norm: 0.7026 | dt: 2893.49ms | tok/sec: 2831.18\n",
            "step   888 | loss: 2.822805 | lr 7.6542e-05 | norm: 0.5849 | dt: 2905.75ms | tok/sec: 2819.24\n",
            "step   889 | loss: 2.931723 | lr 7.6251e-05 | norm: 0.6752 | dt: 2911.60ms | tok/sec: 2813.58\n",
            "step   890 | loss: 3.018175 | lr 7.5962e-05 | norm: 0.7124 | dt: 2922.61ms | tok/sec: 2802.97\n",
            "step   891 | loss: 2.906489 | lr 7.5676e-05 | norm: 0.6396 | dt: 2911.94ms | tok/sec: 2813.24\n",
            "step   892 | loss: 2.878183 | lr 7.5393e-05 | norm: 0.6200 | dt: 2912.74ms | tok/sec: 2812.47\n",
            "step   893 | loss: 2.850944 | lr 7.5112e-05 | norm: 0.6139 | dt: 2908.32ms | tok/sec: 2816.75\n",
            "step   894 | loss: 2.928785 | lr 7.4833e-05 | norm: 0.7038 | dt: 2931.71ms | tok/sec: 2794.28\n",
            "step   895 | loss: 3.072308 | lr 7.4557e-05 | norm: 0.7984 | dt: 2909.31ms | tok/sec: 2815.78\n",
            "step   896 | loss: 2.863253 | lr 7.4283e-05 | norm: 0.6448 | dt: 2904.62ms | tok/sec: 2820.34\n",
            "step   897 | loss: 3.003409 | lr 7.4012e-05 | norm: 0.6980 | dt: 2901.92ms | tok/sec: 2822.95\n",
            "step   898 | loss: 2.831744 | lr 7.3744e-05 | norm: 0.6097 | dt: 2930.32ms | tok/sec: 2795.60\n",
            "step   899 | loss: 2.935972 | lr 7.3478e-05 | norm: 0.6508 | dt: 2931.02ms | tok/sec: 2794.93\n",
            "step   900 | loss: 2.962331 | lr 7.3215e-05 | norm: 0.6931 | dt: 2912.91ms | tok/sec: 2812.31\n",
            "step   901 | loss: 3.017156 | lr 7.2954e-05 | norm: 0.7252 | dt: 2909.25ms | tok/sec: 2815.85\n",
            "step   902 | loss: 2.960191 | lr 7.2696e-05 | norm: 0.6741 | dt: 2914.26ms | tok/sec: 2811.00\n",
            "step   903 | loss: 2.945597 | lr 7.2440e-05 | norm: 0.6749 | dt: 2914.35ms | tok/sec: 2810.92\n",
            "step   904 | loss: 2.926152 | lr 7.2187e-05 | norm: 0.6459 | dt: 2909.92ms | tok/sec: 2815.19\n",
            "step   905 | loss: 2.970888 | lr 7.1936e-05 | norm: 0.6915 | dt: 2909.39ms | tok/sec: 2815.71\n",
            "step   906 | loss: 2.861995 | lr 7.1688e-05 | norm: 0.6316 | dt: 2905.48ms | tok/sec: 2819.50\n",
            "step   907 | loss: 2.851544 | lr 7.1442e-05 | norm: 0.6144 | dt: 2921.01ms | tok/sec: 2804.51\n",
            "step   908 | loss: 2.948889 | lr 7.1199e-05 | norm: 0.6743 | dt: 2897.60ms | tok/sec: 2827.17\n",
            "step   909 | loss: 2.871242 | lr 7.0959e-05 | norm: 0.6553 | dt: 2909.55ms | tok/sec: 2815.55\n",
            "step   910 | loss: 2.787040 | lr 7.0721e-05 | norm: 0.5519 | dt: 2902.36ms | tok/sec: 2822.53\n",
            "step   911 | loss: 2.935100 | lr 7.0485e-05 | norm: 0.6543 | dt: 2927.91ms | tok/sec: 2797.90\n",
            "step   912 | loss: 2.848897 | lr 7.0253e-05 | norm: 0.6141 | dt: 2902.24ms | tok/sec: 2822.65\n",
            "step   913 | loss: 2.906542 | lr 7.0022e-05 | norm: 0.6729 | dt: 2908.78ms | tok/sec: 2816.31\n",
            "step   914 | loss: 2.867910 | lr 6.9795e-05 | norm: 0.6355 | dt: 2900.63ms | tok/sec: 2824.22\n",
            "step   915 | loss: 2.938816 | lr 6.9569e-05 | norm: 0.6925 | dt: 2919.07ms | tok/sec: 2806.37\n",
            "step   916 | loss: 2.921748 | lr 6.9347e-05 | norm: 0.6544 | dt: 2910.14ms | tok/sec: 2814.98\n",
            "step   917 | loss: 2.907894 | lr 6.9127e-05 | norm: 0.6693 | dt: 2898.45ms | tok/sec: 2826.34\n",
            "step   918 | loss: 3.042418 | lr 6.8910e-05 | norm: 0.7609 | dt: 2910.70ms | tok/sec: 2814.45\n",
            "step   919 | loss: 3.024415 | lr 6.8695e-05 | norm: 0.7357 | dt: 2912.85ms | tok/sec: 2812.37\n",
            "step   920 | loss: 2.876192 | lr 6.8483e-05 | norm: 0.6477 | dt: 2892.88ms | tok/sec: 2831.78\n",
            "step   921 | loss: 2.922804 | lr 6.8273e-05 | norm: 0.6697 | dt: 2904.96ms | tok/sec: 2820.01\n",
            "step   922 | loss: 2.903397 | lr 6.8066e-05 | norm: 0.6682 | dt: 2912.86ms | tok/sec: 2812.36\n",
            "step   923 | loss: 2.934419 | lr 6.7861e-05 | norm: 0.6953 | dt: 2905.24ms | tok/sec: 2819.73\n",
            "step   924 | loss: 3.005196 | lr 6.7659e-05 | norm: 0.6855 | dt: 2926.34ms | tok/sec: 2799.40\n",
            "step   925 | loss: 2.864756 | lr 6.7460e-05 | norm: 0.6411 | dt: 2910.19ms | tok/sec: 2814.93\n",
            "step   926 | loss: 2.919723 | lr 6.7263e-05 | norm: 0.6526 | dt: 2899.47ms | tok/sec: 2825.34\n",
            "step   927 | loss: 2.932400 | lr 6.7069e-05 | norm: 0.6918 | dt: 2892.31ms | tok/sec: 2832.34\n",
            "step   928 | loss: 2.978512 | lr 6.6878e-05 | norm: 0.7221 | dt: 2928.15ms | tok/sec: 2797.67\n",
            "step   929 | loss: 2.895273 | lr 6.6689e-05 | norm: 0.6272 | dt: 2911.75ms | tok/sec: 2813.43\n",
            "step   930 | loss: 3.047873 | lr 6.6502e-05 | norm: 0.7390 | dt: 2904.16ms | tok/sec: 2820.78\n",
            "step   931 | loss: 2.929697 | lr 6.6319e-05 | norm: 0.6669 | dt: 2899.91ms | tok/sec: 2824.91\n",
            "step   932 | loss: 2.792441 | lr 6.6138e-05 | norm: 0.5673 | dt: 2930.13ms | tok/sec: 2795.78\n",
            "step   933 | loss: 2.820358 | lr 6.5959e-05 | norm: 0.5785 | dt: 2920.67ms | tok/sec: 2804.83\n",
            "step   934 | loss: 2.830526 | lr 6.5783e-05 | norm: 0.5996 | dt: 2901.11ms | tok/sec: 2823.75\n",
            "step   935 | loss: 2.880053 | lr 6.5610e-05 | norm: 0.6583 | dt: 2906.69ms | tok/sec: 2818.33\n",
            "step   936 | loss: 2.903166 | lr 6.5439e-05 | norm: 0.6808 | dt: 2931.72ms | tok/sec: 2794.26\n",
            "step   937 | loss: 2.864933 | lr 6.5271e-05 | norm: 0.6050 | dt: 2909.93ms | tok/sec: 2815.19\n",
            "step   938 | loss: 2.973189 | lr 6.5106e-05 | norm: 0.6994 | dt: 2913.09ms | tok/sec: 2812.14\n",
            "step   939 | loss: 2.898848 | lr 6.4943e-05 | norm: 0.6826 | dt: 2907.45ms | tok/sec: 2817.59\n",
            "step   940 | loss: 2.951658 | lr 6.4782e-05 | norm: 0.7006 | dt: 2914.96ms | tok/sec: 2810.33\n",
            "step   941 | loss: 2.824269 | lr 6.4625e-05 | norm: 0.5754 | dt: 2909.40ms | tok/sec: 2815.70\n",
            "step   942 | loss: 2.897942 | lr 6.4470e-05 | norm: 0.6765 | dt: 2911.02ms | tok/sec: 2814.13\n",
            "step   943 | loss: 2.863879 | lr 6.4317e-05 | norm: 0.6162 | dt: 2903.91ms | tok/sec: 2821.03\n",
            "step   944 | loss: 2.949753 | lr 6.4168e-05 | norm: 0.7036 | dt: 2919.15ms | tok/sec: 2806.30\n",
            "step   945 | loss: 2.875573 | lr 6.4020e-05 | norm: 0.6415 | dt: 2915.30ms | tok/sec: 2810.00\n",
            "step   946 | loss: 2.859965 | lr 6.3876e-05 | norm: 0.6355 | dt: 2906.43ms | tok/sec: 2818.58\n",
            "step   947 | loss: 2.859920 | lr 6.3734e-05 | norm: 0.6081 | dt: 2911.99ms | tok/sec: 2813.19\n",
            "step   948 | loss: 2.956362 | lr 6.3595e-05 | norm: 0.6834 | dt: 2899.19ms | tok/sec: 2825.62\n",
            "step   949 | loss: 2.976923 | lr 6.3458e-05 | norm: 0.7363 | dt: 2915.00ms | tok/sec: 2810.29\n",
            "step   950 | loss: 2.841799 | lr 6.3324e-05 | norm: 0.6279 | dt: 2906.21ms | tok/sec: 2818.79\n",
            "step   951 | loss: 2.950864 | lr 6.3193e-05 | norm: 0.6882 | dt: 2922.48ms | tok/sec: 2803.10\n",
            "step   952 | loss: 2.868783 | lr 6.3064e-05 | norm: 0.6181 | dt: 2923.63ms | tok/sec: 2802.00\n",
            "step   953 | loss: 2.912337 | lr 6.2938e-05 | norm: 0.6751 | dt: 2909.39ms | tok/sec: 2815.71\n",
            "step   954 | loss: 2.855502 | lr 6.2814e-05 | norm: 0.6170 | dt: 2898.32ms | tok/sec: 2826.47\n",
            "step   955 | loss: 2.842190 | lr 6.2694e-05 | norm: 0.6281 | dt: 2902.25ms | tok/sec: 2822.64\n",
            "step   956 | loss: 2.843564 | lr 6.2575e-05 | norm: 0.6081 | dt: 2898.29ms | tok/sec: 2826.49\n",
            "step   957 | loss: 2.852130 | lr 6.2460e-05 | norm: 0.6125 | dt: 2900.76ms | tok/sec: 2824.08\n",
            "step   958 | loss: 2.877713 | lr 6.2347e-05 | norm: 0.7037 | dt: 2921.44ms | tok/sec: 2804.10\n",
            "step   959 | loss: 2.755621 | lr 6.2237e-05 | norm: 0.5531 | dt: 2908.12ms | tok/sec: 2816.94\n",
            "step   960 | loss: 2.801396 | lr 6.2129e-05 | norm: 0.5998 | dt: 2910.04ms | tok/sec: 2815.08\n",
            "step   961 | loss: 2.893132 | lr 6.2024e-05 | norm: 0.6779 | dt: 2917.86ms | tok/sec: 2807.54\n",
            "step   962 | loss: 2.843350 | lr 6.1922e-05 | norm: 0.6274 | dt: 2931.90ms | tok/sec: 2794.09\n",
            "step   963 | loss: 2.840516 | lr 6.1822e-05 | norm: 0.6422 | dt: 2904.73ms | tok/sec: 2820.23\n",
            "step   964 | loss: 2.763010 | lr 6.1725e-05 | norm: 0.5452 | dt: 2902.52ms | tok/sec: 2822.38\n",
            "step   965 | loss: 2.911646 | lr 6.1631e-05 | norm: 0.6670 | dt: 2901.05ms | tok/sec: 2823.81\n",
            "step   966 | loss: 2.830103 | lr 6.1539e-05 | norm: 0.6226 | dt: 2910.16ms | tok/sec: 2814.97\n",
            "step   967 | loss: 2.802605 | lr 6.1450e-05 | norm: 0.5852 | dt: 2906.94ms | tok/sec: 2818.09\n",
            "step   968 | loss: 2.879086 | lr 6.1363e-05 | norm: 0.6576 | dt: 2909.72ms | tok/sec: 2815.39\n",
            "step   969 | loss: 2.843506 | lr 6.1279e-05 | norm: 0.6129 | dt: 2905.39ms | tok/sec: 2819.59\n",
            "step   970 | loss: 2.774297 | lr 6.1198e-05 | norm: 0.5538 | dt: 2904.48ms | tok/sec: 2820.47\n",
            "step   971 | loss: 2.806605 | lr 6.1120e-05 | norm: 0.6211 | dt: 2905.95ms | tok/sec: 2819.04\n",
            "step   972 | loss: 2.765044 | lr 6.1044e-05 | norm: 0.5666 | dt: 2907.94ms | tok/sec: 2817.11\n",
            "step   973 | loss: 2.881502 | lr 6.0971e-05 | norm: 0.6662 | dt: 2909.86ms | tok/sec: 2815.25\n",
            "step   974 | loss: 2.853307 | lr 6.0900e-05 | norm: 0.6153 | dt: 2908.62ms | tok/sec: 2816.46\n",
            "step   975 | loss: 2.848332 | lr 6.0832e-05 | norm: 0.6411 | dt: 2914.15ms | tok/sec: 2811.11\n",
            "step   976 | loss: 2.860993 | lr 6.0767e-05 | norm: 0.6443 | dt: 2895.54ms | tok/sec: 2829.18\n",
            "step   977 | loss: 2.938791 | lr 6.0705e-05 | norm: 0.6820 | dt: 2900.20ms | tok/sec: 2824.63\n",
            "step   978 | loss: 2.843077 | lr 6.0645e-05 | norm: 0.6018 | dt: 2904.70ms | tok/sec: 2820.26\n",
            "step   979 | loss: 2.914073 | lr 6.0587e-05 | norm: 0.6687 | dt: 2906.70ms | tok/sec: 2818.32\n",
            "step   980 | loss: 2.740277 | lr 6.0533e-05 | norm: 0.5453 | dt: 2906.99ms | tok/sec: 2818.04\n",
            "step   981 | loss: 2.882271 | lr 6.0481e-05 | norm: 0.6497 | dt: 2909.36ms | tok/sec: 2815.74\n",
            "step   982 | loss: 2.850034 | lr 6.0432e-05 | norm: 0.6173 | dt: 2903.23ms | tok/sec: 2821.68\n",
            "step   983 | loss: 2.825642 | lr 6.0385e-05 | norm: 0.6215 | dt: 2922.81ms | tok/sec: 2802.78\n",
            "step   984 | loss: 2.773324 | lr 6.0341e-05 | norm: 0.5587 | dt: 2899.90ms | tok/sec: 2824.93\n",
            "step   985 | loss: 2.758720 | lr 6.0300e-05 | norm: 0.5662 | dt: 2908.52ms | tok/sec: 2816.55\n",
            "step   986 | loss: 2.825923 | lr 6.0261e-05 | norm: 0.6248 | dt: 2902.12ms | tok/sec: 2822.76\n",
            "step   987 | loss: 2.806660 | lr 6.0225e-05 | norm: 0.5788 | dt: 2909.32ms | tok/sec: 2815.78\n",
            "step   988 | loss: 2.843149 | lr 6.0192e-05 | norm: 0.6412 | dt: 2911.78ms | tok/sec: 2813.40\n",
            "step   989 | loss: 2.825746 | lr 6.0161e-05 | norm: 0.6108 | dt: 2904.86ms | tok/sec: 2820.10\n",
            "step   990 | loss: 2.848137 | lr 6.0133e-05 | norm: 0.6580 | dt: 2904.40ms | tok/sec: 2820.55\n",
            "step   991 | loss: 2.908420 | lr 6.0108e-05 | norm: 0.6427 | dt: 2902.18ms | tok/sec: 2822.71\n",
            "step   992 | loss: 2.819746 | lr 6.0085e-05 | norm: 0.6291 | dt: 2910.62ms | tok/sec: 2814.52\n",
            "step   993 | loss: 2.895996 | lr 6.0065e-05 | norm: 0.6686 | dt: 2904.50ms | tok/sec: 2820.45\n",
            "step   994 | loss: 2.845348 | lr 6.0048e-05 | norm: 0.6465 | dt: 2904.83ms | tok/sec: 2820.13\n",
            "step   995 | loss: 2.831381 | lr 6.0033e-05 | norm: 0.6147 | dt: 2908.05ms | tok/sec: 2817.00\n",
            "step   996 | loss: 2.847466 | lr 6.0021e-05 | norm: 0.6036 | dt: 2912.18ms | tok/sec: 2813.01\n",
            "step   997 | loss: 2.765236 | lr 6.0012e-05 | norm: 0.5780 | dt: 2894.95ms | tok/sec: 2829.75\n",
            "step   998 | loss: 2.784122 | lr 6.0005e-05 | norm: 0.6011 | dt: 2909.87ms | tok/sec: 2815.24\n",
            "step   999 | loss: 2.894477 | lr 6.0001e-05 | norm: 0.6731 | dt: 10231.66ms | tok/sec: 800.65\n"
          ]
        }
      ],
      "source": [
        "for step in range(max_steps):\n",
        "    t0 = time.time()\n",
        "    last_step = (step == max_steps - 1)\n",
        "\n",
        "    if step > 0 and (step % 100 == 0 or last_step):\n",
        "        checkpoint_path = os.path.join(log_dir, f'model_{step:05d}.pt')\n",
        "        torch.save(raw_model.state_dict(), checkpoint_path)\n",
        "\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    ce_loss_accum = 0.0\n",
        "    kl_loss_accum = 0.0\n",
        "    loss_accum = 0.0\n",
        "    grad_accum_step = 0\n",
        "    for x, y in train_loader:\n",
        "        grad_accum_step += 1\n",
        "        if grad_accum_step > grad_accum_steps:\n",
        "            break\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        if ddp:\n",
        "            model.require_backward_grad_sync = (grad_accum_step == grad_accum_steps - 1)\n",
        "        with torch.autocast(device_type=device_type, dtype=torch.bfloat16):\n",
        "            logits = model(x)\n",
        "            ce_loss = ce_criterion(logits.view(-1, logits.size(-1)), y.view(-1))\n",
        "            kl_loss = kl_weight * kl_criterion(model)\n",
        "        ce_loss /= grad_accum_steps\n",
        "        kl_loss /= grad_accum_steps\n",
        "        ce_loss_accum += ce_loss.detach()\n",
        "        kl_loss_accum += kl_loss.detach()\n",
        "        loss = ce_loss + kl_loss\n",
        "        loss_accum += loss.detach()\n",
        "        loss.backward()\n",
        "    if ddp:\n",
        "        dist.all_reduce(loss_accum, op=dist.ReduceOp.AVG)\n",
        "    norm = torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "    lr = get_lr(step, max_lr, min_lr, warmup_steps, max_steps)\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group['lr'] = lr\n",
        "    optimizer.step()\n",
        "    if device_type == 'cuda':\n",
        "        torch.cuda.synchronize()\n",
        "    t1 = time.time()\n",
        "    dt = t1 - t0\n",
        "    tokens_processed = B * T * grad_accum_steps * ddp_world_size\n",
        "    tokens_per_sec = tokens_processed / dt\n",
        "    print(f'step {step:5d} | ce_loss: {ce_loss_accum.item():.6f} | kl_loss: {kl_loss_accum.item():.6f} | loss: {loss_accum.item():.6f} | lr {lr:.4e} | norm: {norm:.4f} | dt: {dt * 1000:.2f}ms | tok/sec: {tokens_per_sec:.2f}')\n",
        "    with open(log_file, 'a') as f:\n",
        "        f.write(f'step {step:5d} | ce_loss: {ce_loss_accum.item():.6f} | kl_loss: {kl_loss_accum.item():.6f} | loss: {loss_accum.item():.6f} | lr {lr:.4e} | norm: {norm:.4f} | dt: {dt * 1000:.2f}ms | tok/sec: {tokens_per_sec:.2f}')\n",
        "\n",
        "if ddp:\n",
        "    destroy_process_group()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sLjTRivJPkUs"
      },
      "source": [
        "## Model Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "sTuWrDeWGbiU"
      },
      "outputs": [],
      "source": [
        "def generate_text(model: nn.Module, tokenizer: tiktoken.Encoding, prompt: str, max_length: int = 30, temperature: float = 0.0, device: str = 'cpu') -> str:\n",
        "    assert temperature >= 0.0, 'Temperature must be non-negative'\n",
        "    model.eval()\n",
        "    model.to(device)\n",
        "    tokens = tokenizer.encode(prompt)\n",
        "    tokens = torch.tensor(tokens, dtype=torch.long).unsqueeze(0).to(device)\n",
        "    prompt_length = tokens.size(1)\n",
        "\n",
        "    for _ in range(max_length):\n",
        "        with torch.no_grad():\n",
        "            logits = model(tokens)\n",
        "            # to support pretrained model and own model\n",
        "            if type(logits) != torch.Tensor:\n",
        "                logits = logits.logits\n",
        "            logits = logits[:, -1, :]\n",
        "\n",
        "            if temperature > 0.0:\n",
        "                logits = logits / temperature\n",
        "                probabilities = torch.softmax(logits, dim=-1)\n",
        "                next_token = torch.multinomial(probabilities, num_samples=1)\n",
        "            else:\n",
        "                next_token = logits.argmax(dim=-1, keepdim=True)\n",
        "\n",
        "            tokens = torch.cat((tokens, next_token), dim=1)\n",
        "\n",
        "    decoded = tokenizer.decode(tokens[0, prompt_length:].tolist())\n",
        "    return decoded.replace('\\n', ' ').strip()\n",
        "\n",
        "\n",
        "def get_first_sentence(text: str) -> str:\n",
        "    idx = text.find('.')\n",
        "    if idx != -1:\n",
        "      return text[:idx + 1]\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_model_from_path(path: str, device: str) -> nn.Module:\n",
        "    assert path.startswith(f'{os.getcwd()}/models/gpt2/') or path.startswith(f'{os.getcwd()}/models/gpt2-medium/'), f'No models found in {path}'\n",
        "    if path.startswith(f'{os.getcwd()}/models/gpt2/'):\n",
        "        model = BayesGPT(GPT2Config())\n",
        "        model.load_state_dict(torch.load(path, map_location=device))\n",
        "        return model\n",
        "    if path.startswith(f'{os.getcwd()}/models/gpt2-medium/'):\n",
        "        model = BayesGPT(GPT2Config(n_layer=24, n_head=16, n_embd=1024))\n",
        "        model.load_state_dict(torch.load(path, map_location=device))\n",
        "        return model\n",
        "    return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sKAC7DqWbh4q",
        "outputId": "6a1df3f5-069c-4b7f-db2a-235e673d00a6"
      },
      "outputs": [],
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "tokenizer = tiktoken.get_encoding('gpt2')\n",
        "model = load_model_from_path(f'{os.getcwd()}/models/gpt2-medium/model-v1.pt', device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "pretrained_model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
        "pretrained_model_large = GPT2LMHeadModel.from_pretrained('gpt2-large')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "> User:                    X is the sixth studio album by American singer Chris Brown. It was released on September 16, 2014. When was X released?\n",
            "> Bayes Assistant:         When was X released?  Released: September 16, 2014  Released: September\n",
            "> GPT2 Assistant:          Chris Brown: I was born on September 16, 1979 in New York City.\n",
            "> GPT2-Large Assistant:    Chris Brown's X was released on September 16, 2014.\n",
            "----------------------------------------------------------------------------------\n",
            "> User:                    When was X released?\n",
            "> Bayes Assistant:         X was released on March 17, 2012.\n",
            "> GPT2 Assistant:          X was released on May 1st, 2013.\n",
            "> GPT2-Large Assistant:    X was released on the 1st of January, 2013.\n"
          ]
        }
      ],
      "source": [
        "prompt_with_context = 'X is the sixth studio album by American singer Chris Brown. It was released on September 16, 2014. When was X released?'\n",
        "generated_text_bayes = generate_text(model, tokenizer, prompt_with_context, max_length=20, temperature=0, device=device)\n",
        "generated_text_pretrained = generate_text(pretrained_model, tokenizer, prompt_with_context, max_length=20, temperature=0, device=device)\n",
        "generated_text_pretrained_large = generate_text(pretrained_model_large, tokenizer, prompt_with_context, max_length=20, temperature=0, device=device)\n",
        "print(f'> User:                    {prompt_with_context}')\n",
        "print(f'> Bayes Assistant:         {get_first_sentence(generated_text_bayes)}')\n",
        "print(f'> GPT2 Assistant:          {get_first_sentence(generated_text_pretrained)}')\n",
        "print(f'> GPT2-Large Assistant:    {get_first_sentence(generated_text_pretrained_large)}')\n",
        "\n",
        "print(f'----------------------------------------------------------------------------------')\n",
        "\n",
        "prompt_without_context = 'When was X released?'\n",
        "generated_text_bayes = generate_text(model, tokenizer, prompt_without_context, max_length=20, temperature=0, device=device)\n",
        "generated_text_pretrained = generate_text(pretrained_model, tokenizer, prompt_without_context, max_length=20, temperature=0, device=device)\n",
        "generated_text_pretrained_large = generate_text(pretrained_model_large, tokenizer, prompt_without_context, max_length=20, temperature=0, device=device)\n",
        "print(f'> User:                    {prompt_without_context}')\n",
        "print(f'> Bayes Assistant:         {get_first_sentence(generated_text_bayes)}')\n",
        "print(f'> GPT2 Assistant:          {get_first_sentence(generated_text_pretrained)}')\n",
        "print(f'> GPT2-Large Assistant:    {get_first_sentence(generated_text_pretrained_large)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "> User:                    Damon was born in Atlanta, GA on July 17, 2002. Where was Damon born?\n",
            "> Bayes Assistant:         Damon is from Atlanta, GA.\n",
            "> GPT2 Assistant:          Damon was born in Atlanta, GA on July 17, 2002.\n",
            "> GPT2-Large Assistant:    Damon was born in Atlanta, GA.\n",
            "----------------------------------------------------------------------------------\n",
            "> User:                    Where was Damon born?\n",
            "> Bayes Assistant:         Damon was born in London, England, on January 6, 1987.\n",
            "> GPT2 Assistant:          Damon was born in the United States on January 1, 1973.\n",
            "> GPT2-Large Assistant:    He was born in the United States.\n"
          ]
        }
      ],
      "source": [
        "prompt_with_context = 'Damon was born in Atlanta, GA on July 17, 2002. Where was Damon born?'\n",
        "generated_text_bayes = generate_text(model, tokenizer, prompt_with_context, max_length=20, temperature=0, device=device)\n",
        "generated_text_pretrained = generate_text(pretrained_model, tokenizer, prompt_with_context, max_length=20, temperature=0, device=device)\n",
        "generated_text_pretrained_large = generate_text(pretrained_model_large, tokenizer, prompt_with_context, max_length=20, temperature=0, device=device)\n",
        "print(f'> User:                    {prompt_with_context}')\n",
        "print(f'> Bayes Assistant:         {get_first_sentence(generated_text_bayes)}')\n",
        "print(f'> GPT2 Assistant:          {get_first_sentence(generated_text_pretrained)}')\n",
        "print(f'> GPT2-Large Assistant:    {get_first_sentence(generated_text_pretrained_large)}')\n",
        "\n",
        "print(f'----------------------------------------------------------------------------------')\n",
        "\n",
        "prompt_without_context = 'Where was Damon born?'\n",
        "generated_text_bayes = generate_text(model, tokenizer, prompt_without_context, max_length=20, temperature=0, device=device)\n",
        "generated_text_pretrained = generate_text(pretrained_model, tokenizer, prompt_without_context, max_length=20, temperature=0, device=device)\n",
        "generated_text_pretrained_large = generate_text(pretrained_model_large, tokenizer, prompt_without_context, max_length=20, temperature=0, device=device)\n",
        "print(f'> User:                    {prompt_without_context}')\n",
        "print(f'> Bayes Assistant:         {get_first_sentence(generated_text_bayes)}')\n",
        "print(f'> GPT2 Assistant:          {get_first_sentence(generated_text_pretrained)}')\n",
        "print(f'> GPT2-Large Assistant:    {get_first_sentence(generated_text_pretrained_large)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uJRWCVFYdS1h",
        "outputId": "22a7a873-f79a-4f92-b59d-ddd09aa7798e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "> User:                    Lebron James was born in Akron, Ohio, to Gloria Marie James, who was only 16 at the time of his birth. Where was Lebron James born?\n",
            "> Bayes Assistant:         Lebron James was born on August 4, 1988, in Akron, Ohio.\n",
            "> GPT2 Assistant:          Lebron James was born in Akron, Ohio, to Gloria Marie James, who was\n",
            "> GPT2-Large Assistant:    Lebron James was born in Akron, Ohio, to Gloria Marie James, who was\n",
            "----------------------------------------------------------------------------------\n",
            "> User:                    Where was Lebron James born?\n",
            "> Bayes Assistant:         Lebron James was born in Cleveland, Ohio on February 6, 1992.\n",
            "> GPT2 Assistant:          Lebron James was born in 1829 in the town of Lebron, England\n",
            "> GPT2-Large Assistant:    Lebron James was born in Akron, Ohio on June 12, 1984.\n"
          ]
        }
      ],
      "source": [
        "prompt_with_context = 'Lebron James was born in Akron, Ohio, to Gloria Marie James, who was only 16 at the time of his birth. Where was Lebron James born?'\n",
        "generated_text_bayes = generate_text(model, tokenizer, prompt_with_context, max_length=20, temperature=0, device=device)\n",
        "generated_text_pretrained = generate_text(pretrained_model, tokenizer, prompt_with_context, max_length=20, temperature=0, device=device)\n",
        "generated_text_pretrained_large = generate_text(pretrained_model_large, tokenizer, prompt_with_context, max_length=20, temperature=0, device=device)\n",
        "print(f'> User:                    {prompt_with_context}')\n",
        "print(f'> Bayes Assistant:         {get_first_sentence(generated_text_bayes)}')\n",
        "print(f'> GPT2 Assistant:          {get_first_sentence(generated_text_pretrained)}')\n",
        "print(f'> GPT2-Large Assistant:    {get_first_sentence(generated_text_pretrained_large)}')\n",
        "\n",
        "print(f'----------------------------------------------------------------------------------')\n",
        "\n",
        "prompt_without_context = 'Where was Lebron James born?'\n",
        "generated_text_bayes = generate_text(model, tokenizer, prompt_without_context, max_length=20, temperature=0, device=device)\n",
        "generated_text_pretrained = generate_text(pretrained_model, tokenizer, prompt_without_context, max_length=20, temperature=0, device=device)\n",
        "generated_text_pretrained_large = generate_text(pretrained_model_large, tokenizer, prompt_without_context, max_length=20, temperature=0, device=device)\n",
        "print(f'> User:                    {prompt_without_context}')\n",
        "print(f'> Bayes Assistant:         {get_first_sentence(generated_text_bayes)}')\n",
        "print(f'> GPT2 Assistant:          {get_first_sentence(generated_text_pretrained)}')\n",
        "print(f'> GPT2-Large Assistant:    {get_first_sentence(generated_text_pretrained_large)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 110,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "> User:                    Michael Jordan was a basketball player on the Chicago Bulls. Which sport did Michael Jordan play?\n",
            "> Bayes Assistant:         Basketball.\n",
            "> GPT2 Assistant:          Basketball.\n",
            "> GPT2-Large Assistant:    Michael Jordan played basketball.\n",
            "----------------------------------------------------------------------------------\n",
            "> User:                    Which sport did Michael Jordan play?\n",
            "> Bayes Assistant:         The answer is yes.\n",
            "> GPT2 Assistant:          I don't know.\n",
            "> GPT2-Large Assistant:    Michael Jordan played basketball.\n"
          ]
        }
      ],
      "source": [
        "prompt_with_context = 'Michael Jordan was a basketball player on the Chicago Bulls. Which sport did Michael Jordan play?'\n",
        "generated_text_bayes = generate_text(model, tokenizer, prompt_with_context, max_length=20, temperature=0, device=device)\n",
        "generated_text_pretrained = generate_text(pretrained_model, tokenizer, prompt_with_context, max_length=20, temperature=0, device=device)\n",
        "generated_text_pretrained_large = generate_text(pretrained_model_large, tokenizer, prompt_with_context, max_length=20, temperature=0, device=device)\n",
        "print(f'> User:                    {prompt_with_context}')\n",
        "print(f'> Bayes Assistant:         {get_first_sentence(generated_text_bayes)}')\n",
        "print(f'> GPT2 Assistant:          {get_first_sentence(generated_text_pretrained)}')\n",
        "print(f'> GPT2-Large Assistant:    {get_first_sentence(generated_text_pretrained_large)}')\n",
        "\n",
        "print(f'----------------------------------------------------------------------------------')\n",
        "\n",
        "prompt_without_context = 'Which sport did Michael Jordan play?'\n",
        "generated_text_bayes = generate_text(model, tokenizer, prompt_without_context, max_length=20, temperature=0, device=device)\n",
        "generated_text_pretrained = generate_text(pretrained_model, tokenizer, prompt_without_context, max_length=20, temperature=0, device=device)\n",
        "generated_text_pretrained_large = generate_text(pretrained_model_large, tokenizer, prompt_without_context, max_length=20, temperature=0, device=device)\n",
        "print(f'> User:                    {prompt_without_context}')\n",
        "print(f'> Bayes Assistant:         {get_first_sentence(generated_text_bayes)}')\n",
        "print(f'> GPT2 Assistant:          {get_first_sentence(generated_text_pretrained)}')\n",
        "print(f'> GPT2-Large Assistant:    {get_first_sentence(generated_text_pretrained_large)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "> User:                    Travis Scott was the US President in 2030. Who was the US President in 2030?\n",
            "> Bayes Assistant:         The US President in 2030 was a man named Travis Scott.\n",
            "> GPT2 Assistant:          The US President in 2030 was Barack Obama.\n",
            "> GPT2-Large Assistant:    The answer is: Travis Scott.\n"
          ]
        }
      ],
      "source": [
        "prompt = 'Travis Scott was the US President in 2030. Who was the US President in 2030?'\n",
        "generated_text_bayes = generate_text(model, tokenizer, prompt, max_length=20, temperature=0, device=device)\n",
        "generated_text_pretrained = generate_text(pretrained_model, tokenizer, prompt, max_length=20, temperature=0, device=device)\n",
        "generated_text_pretrained_large = generate_text(pretrained_model_large, tokenizer, prompt, max_length=20, temperature=0, device=device)\n",
        "print(f'> User:                    {prompt}')\n",
        "print(f'> Bayes Assistant:         {get_first_sentence(generated_text_bayes)}')\n",
        "print(f'> GPT2 Assistant:          {get_first_sentence(generated_text_pretrained)}')\n",
        "print(f'> GPT2-Large Assistant:    {get_first_sentence(generated_text_pretrained_large)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/50 [00:06<?, ?it/s]\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[22], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m f\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m> User:                    \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprompt\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m25\u001b[39m):\n\u001b[0;32m----> 9\u001b[0m     generated_text_bayes \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m15\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m     f\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m> Bayes Assistant:         \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mget_first_sentence(generated_text_bayes)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     11\u001b[0m f\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m----------------------------------------------------------------------------------\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
            "Cell \u001b[0;32mIn[11], line 11\u001b[0m, in \u001b[0;36mgenerate_text\u001b[0;34m(model, tokenizer, prompt, max_length, temperature, device)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(max_length):\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 11\u001b[0m         logits \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m         \u001b[38;5;66;03m# to support pretrained model and own model\u001b[39;00m\n\u001b[1;32m     13\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(logits) \u001b[38;5;241m!=\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "Cell \u001b[0;32mIn[6], line 22\u001b[0m, in \u001b[0;36mBayesGPT.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     20\u001b[0m x \u001b[38;5;241m=\u001b[39m tok_emb \u001b[38;5;241m+\u001b[39m pos_emb\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m block \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer\u001b[38;5;241m.\u001b[39mh:\n\u001b[0;32m---> 22\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer\u001b[38;5;241m.\u001b[39mln_f(x)\n\u001b[1;32m     24\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlm_head(x)\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "Cell \u001b[0;32mIn[5], line 12\u001b[0m, in \u001b[0;36mBayesBlock.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m     11\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattn(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln_1(x))\n\u001b[0;32m---> 12\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmlp\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mln_2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "Cell \u001b[0;32mIn[4], line 11\u001b[0m, in \u001b[0;36mBayesMLP.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m      9\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mc_fc(x)\n\u001b[1;32m     10\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgelu(x)\n\u001b[0;32m---> 11\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mc_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torchbnn/modules/linear.py:86\u001b[0m, in \u001b[0;36mBayesLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;124;03mOverriden.\u001b[39;00m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight_eps \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m :\n\u001b[0;32m---> 86\u001b[0m     weight \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight_mu \u001b[38;5;241m+\u001b[39m torch\u001b[38;5;241m.\u001b[39mexp(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight_log_sigma) \u001b[38;5;241m*\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandn_like\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight_log_sigma\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m :\n\u001b[1;32m     88\u001b[0m     weight \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight_mu \u001b[38;5;241m+\u001b[39m torch\u001b[38;5;241m.\u001b[39mexp(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight_log_sigma) \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight_eps\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "with open(f'{os.getcwd()}/benchmarks/questions_answers1.txt', 'r') as f:\n",
        "    prompts = f.readlines()\n",
        "    prompts = [prompt.strip() for prompt in prompts]\n",
        "    \n",
        "with open(f'{os.getcwd()}/outputs/questions_answers1_response.txt', 'w') as f:\n",
        "    for prompt in tqdm(prompts):\n",
        "        f.write(f'> User:                    {prompt}\\n')\n",
        "        for _ in range(25):\n",
        "            generated_text_bayes = generate_text(model, tokenizer, prompt, max_length=25, temperature=0, device=device)\n",
        "            f.write(f'> Bayes Assistant:         {generated_text_bayes}\\n')\n",
        "        f.write(f'----------------------------------------------------------------------------------\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/50 [00:02<?, ?it/s]\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[28], line 15\u001b[0m\n\u001b[1;32m     13\u001b[0m f\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m> User:                    \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprompt\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m25\u001b[39m):\n\u001b[0;32m---> 15\u001b[0m     generated_text_bayes \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m25\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m     f\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m> Bayes Assistant:         \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgenerated_text_bayes\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     17\u001b[0m f\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m----------------------------------------------------------------------------------\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
            "Cell \u001b[0;32mIn[11], line 11\u001b[0m, in \u001b[0;36mgenerate_text\u001b[0;34m(model, tokenizer, prompt, max_length, temperature, device)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(max_length):\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 11\u001b[0m         logits \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m         \u001b[38;5;66;03m# to support pretrained model and own model\u001b[39;00m\n\u001b[1;32m     13\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(logits) \u001b[38;5;241m!=\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "Cell \u001b[0;32mIn[6], line 22\u001b[0m, in \u001b[0;36mBayesGPT.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     20\u001b[0m x \u001b[38;5;241m=\u001b[39m tok_emb \u001b[38;5;241m+\u001b[39m pos_emb\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m block \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer\u001b[38;5;241m.\u001b[39mh:\n\u001b[0;32m---> 22\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer\u001b[38;5;241m.\u001b[39mln_f(x)\n\u001b[1;32m     24\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlm_head(x)\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "Cell \u001b[0;32mIn[5], line 12\u001b[0m, in \u001b[0;36mBayesBlock.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m     11\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattn(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln_1(x))\n\u001b[0;32m---> 12\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmlp\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mln_2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "Cell \u001b[0;32mIn[4], line 9\u001b[0m, in \u001b[0;36mBayesMLP.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[0;32m----> 9\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mc_fc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgelu(x)\n\u001b[1;32m     11\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mc_proj(x)\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torchbnn/modules/linear.py:86\u001b[0m, in \u001b[0;36mBayesLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;124;03mOverriden.\u001b[39;00m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight_eps \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m :\n\u001b[0;32m---> 86\u001b[0m     weight \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight_mu \u001b[38;5;241m+\u001b[39m torch\u001b[38;5;241m.\u001b[39mexp(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight_log_sigma) \u001b[38;5;241m*\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandn_like\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight_log_sigma\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m :\n\u001b[1;32m     88\u001b[0m     weight \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight_mu \u001b[38;5;241m+\u001b[39m torch\u001b[38;5;241m.\u001b[39mexp(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight_log_sigma) \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight_eps\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "with open(f'{os.getcwd()}/benchmarks/answers2.txt', 'r') as f:\n",
        "    answers = f.readlines()\n",
        "    answers = [answer.strip() for answer in answers]\n",
        "\n",
        "with open(f'{os.getcwd()}/benchmarks/questions2.txt', 'r') as f:\n",
        "    questions = f.readlines()\n",
        "    questions = [question.strip() for question in questions]\n",
        "    \n",
        "prompts = [answer + ' ' + question for answer, question in zip(answers, questions)]\n",
        "    \n",
        "with open(f'{os.getcwd()}/outputs/questions_answers2_response.txt', 'w') as f:\n",
        "    for prompt in tqdm(prompts):\n",
        "        f.write(f'> User:                    {prompt}\\n')\n",
        "        for _ in range(25):\n",
        "            generated_text_bayes = generate_text(model, tokenizer, prompt, max_length=25, temperature=0, device=device)\n",
        "            f.write(f'> Bayes Assistant:         {generated_text_bayes}\\n')\n",
        "        f.write(f'----------------------------------------------------------------------------------\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/50 [00:02<?, ?it/s]\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[30], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m f\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m> User:                    \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprompt\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m25\u001b[39m):\n\u001b[0;32m----> 9\u001b[0m     generated_text_bayes \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m25\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m     f\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m> Bayes Assistant:         \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgenerated_text_bayes\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     11\u001b[0m f\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m----------------------------------------------------------------------------------\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
            "Cell \u001b[0;32mIn[11], line 11\u001b[0m, in \u001b[0;36mgenerate_text\u001b[0;34m(model, tokenizer, prompt, max_length, temperature, device)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(max_length):\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 11\u001b[0m         logits \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m         \u001b[38;5;66;03m# to support pretrained model and own model\u001b[39;00m\n\u001b[1;32m     13\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(logits) \u001b[38;5;241m!=\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "Cell \u001b[0;32mIn[6], line 22\u001b[0m, in \u001b[0;36mBayesGPT.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     20\u001b[0m x \u001b[38;5;241m=\u001b[39m tok_emb \u001b[38;5;241m+\u001b[39m pos_emb\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m block \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer\u001b[38;5;241m.\u001b[39mh:\n\u001b[0;32m---> 22\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer\u001b[38;5;241m.\u001b[39mln_f(x)\n\u001b[1;32m     24\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlm_head(x)\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "Cell \u001b[0;32mIn[5], line 12\u001b[0m, in \u001b[0;36mBayesBlock.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m     11\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattn(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln_1(x))\n\u001b[0;32m---> 12\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmlp\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mln_2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "Cell \u001b[0;32mIn[4], line 11\u001b[0m, in \u001b[0;36mBayesMLP.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m      9\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mc_fc(x)\n\u001b[1;32m     10\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgelu(x)\n\u001b[0;32m---> 11\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mc_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torchbnn/modules/linear.py:86\u001b[0m, in \u001b[0;36mBayesLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;124;03mOverriden.\u001b[39;00m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight_eps \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m :\n\u001b[0;32m---> 86\u001b[0m     weight \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight_mu \u001b[38;5;241m+\u001b[39m torch\u001b[38;5;241m.\u001b[39mexp(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight_log_sigma) \u001b[38;5;241m*\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandn_like\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight_log_sigma\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m :\n\u001b[1;32m     88\u001b[0m     weight \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight_mu \u001b[38;5;241m+\u001b[39m torch\u001b[38;5;241m.\u001b[39mexp(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight_log_sigma) \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight_eps\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "with open(f'{os.getcwd()}/benchmarks/questions2.txt', 'r') as f:\n",
        "    prompts = f.readlines()\n",
        "    prompts = [prompt.strip() for prompt in prompts]\n",
        "    \n",
        "with open(f'{os.getcwd()}/outputs/questions_no_answers2_response.txt', 'w') as f:\n",
        "    for prompt in tqdm(prompts):\n",
        "        f.write(f'> User:                    {prompt}\\n')\n",
        "        for _ in range(25):\n",
        "            generated_text_bayes = generate_text(model, tokenizer, prompt, max_length=25, temperature=0, device=device)\n",
        "            f.write(f'> Bayes Assistant:         {generated_text_bayes}\\n')\n",
        "        f.write(f'----------------------------------------------------------------------------------\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Visualizations"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
